{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from importlib import reload\n",
    "import warnings\n",
    "os.environ['USE_PYGEOS'] = '0'\n",
    "\n",
    "import geopandas as gpd\n",
    "from sqlalchemy import create_engine\n",
    "import libpysal as lps\n",
    "from esda import smoothing as sm\n",
    "from libpysal.weights.distance import get_points_array\n",
    "from scipy.spatial import cKDTree\n",
    "from spreg import OLS\n",
    "from esda import fdr\n",
    "import spreg\n",
    "from sklearn.tree import DecisionTreeClassifier, export_graphviz\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics, preprocessing\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, minmax_scale\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib_scalebar.scalebar import ScaleBar\n",
    "from shapely.geometry import Point, Polygon\n",
    "from matplotlib import patches, colors\n",
    "from matplotlib.patches import Rectangle\n",
    "from matplotlib.collections import PatchCollection\n",
    "from matplotlib.colors import rgb2hex\n",
    "import matplotlib.dates as mdates\n",
    "import matplotlib.ticker as mticker\n",
    "from PIL import ImageColor\n",
    "import matplotlib_inline.backend_inline\n",
    "import pydotplus\n",
    "import plotly.express as px\n",
    "import pickle\n",
    "from generativepy.color import Color\n",
    "# from geoalchemy2 import Geometry, WKTElement\n",
    "import contextily as ctx\n",
    "from six import StringIO\n",
    "from tableone import TableOne\n",
    "# from statannotations.Annotator import Annotator\n",
    "\n",
    "sys.path.append('/Users/david/Dropbox/PhD/Scripts/Spatial analyses')\n",
    "import pyspace\n",
    "from utils import *\n",
    "engine = create_engine('postgresql://postgres@localhost:5432/david')\n",
    "# Set pandas display options\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "# Ignore this specific UserWarning\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, message=\"The weights matrix is not fully connected\")\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "# Set matplotlib display format\n",
    "# matplotlib_inline.backend_inline.set_matplotlib_formats('retina')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import esda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set working directory\n",
    "mydir = Path(os.getcwd())\n",
    "data_folder = mydir / '../Data' # Set data folder\n",
    "result_folder = mydir / '../Results' # Set results folder\n",
    "figures_folder = Path('../Manuscript/Figures - EN/') # Set figures folder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "## Import data\n",
    "### BCS participation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Participation data\n",
    "df_participation = pd.read_feather('../Data/Processed data/20230627_GIRACS_all.feather')\n",
    "gdf_participation = gpd.read_feather('../Data/Processed data/20230627_GIRACS_all.feather')\n",
    "\n",
    "print('Number of invitations :', gdf_participation.shape[0])\n",
    "print('Number of women :', gdf_participation.numerodossier.nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_participation[(gdf_participation['year_invit']>2002)].numerodossier.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "### Administrative units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Administrative boundaries\n",
    "lake = gpd.read_file('/Users/david/Dropbox/PhD/GitHub/COVID19/Data/Mapping/lake.geojson')\n",
    "lake.NOM = ['Lake Geneva', '', '']\n",
    "cantons = gpd.read_file(\n",
    "    '/Users/david/Dropbox/PhD/Data/Databases/SITG/SHAPEFILE_LV95_LN02/swissBOUNDARIES3D_1_3_TLM_KANTONSGEBIET.shp')\n",
    "communes_ch = gpd.read_file(\n",
    "    '/Users/david/Dropbox/PhD/Data/Databases/SITG/SHAPEFILE_LV95_LN02/swissBOUNDARIES3D_1_3_TLM_HOHEITSGEBIET.shp')\n",
    "# Only retain communes that are in the canton of Geneva\n",
    "communes = communes_ch[communes_ch.KANTONSNUM == 25]\n",
    "\n",
    "cantons = cantons.to_crs(2056)\n",
    "communes_ch = communes_ch.to_crs(2056)\n",
    "communes = communes.to_crs(2056)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "## Study setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.patches as patches\n",
    "from matplotlib.patches import ConnectionPatch\n",
    "import numpy as np\n",
    "from shapely.geometry import box\n",
    "import contextily as ctx\n",
    "from matplotlib_scalebar.scalebar import ScaleBar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "communes[communes.NAME=='Genève'].geometry.centroid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_geneva_context_map(cantons, communes):\n",
    "    \"\"\"\n",
    "    Create a context map showing Geneva canton within Switzerland,\n",
    "    with an inset map showing Geneva's municipalities.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    cantons : geopandas.GeoDataFrame\n",
    "        GeoDataFrame containing all Swiss cantons with 'NAME' column\n",
    "    communes : geopandas.GeoDataFrame  \n",
    "        GeoDataFrame containing Geneva municipalities with 'NAME' column\n",
    "    \"\"\"\n",
    "    \n",
    "    # === DATA PREPARATION ===\n",
    "    # Ensure both GeoDataFrames are in WGS84 (EPSG:4326) for plotting\n",
    "    # if cantons.crs != 'EPSG:4326':\n",
    "    #     cantons = cantons.to_crs('EPSG:4326')\n",
    "    # if communes.crs != 'EPSG:4326':\n",
    "    #     communes = communes.to_crs('EPSG:4326')\n",
    "    fig = plt.figure(figsize=(12, 8))\n",
    "    # Extract Geneva canton\n",
    "    geneva_canton = cantons[cantons['NAME'] == 'Genève'].copy()\n",
    "    if geneva_canton.empty:\n",
    "        raise ValueError(\"Geneva canton not found. Check that 'NAME' column contains 'Genève'\")\n",
    "    \n",
    "    # Extract Geneva city (most populated municipality)\n",
    "    geneva_city = communes[communes['NAME'] == 'Genève'].copy()\n",
    "    if geneva_city.empty:\n",
    "        print(\"Warning: Geneva city not found in communes dataset\")\n",
    "        geneva_city = None\n",
    "    \n",
    "    # Main Switzerland map (left side, larger)\n",
    "    ax_main = plt.subplot2grid((2, 3), (0, 0), colspan=2, rowspan=2)\n",
    "    \n",
    "    # Geneva detail map (right side, smaller)\n",
    "    ax_detail = plt.subplot2grid((2, 4), (0, 3), rowspan=1)\n",
    "    \n",
    "    # === MAIN SWITZERLAND MAP ===\n",
    "    # ax_main.set_title('Switzerland - Canton of Geneva Location', \n",
    "    #                  fontsize=14, fontweight='bold', pad=20)\n",
    "    \n",
    "    # Plot all Swiss cantons\n",
    "    cantons.plot(ax=ax_main, color='lightgray', edgecolor='white', linewidth=0.5)\n",
    "    \n",
    "    # Highlight Geneva canton\n",
    "    geneva_canton.plot(ax=ax_main, color='red', alpha=0.7, edgecolor='darkred', linewidth=2)\n",
    "    \n",
    "    # Get Geneva bounds for boundary rectangle and connection\n",
    "    geneva_bounds = geneva_canton.total_bounds  # [minx, miny, maxx, maxy]\n",
    "    \n",
    "    # Add boundary rectangle around Geneva for connection\n",
    "    boundary_rect = patches.Rectangle((geneva_bounds[0], geneva_bounds[1]), \n",
    "                                     geneva_bounds[2] - geneva_bounds[0],\n",
    "                                     geneva_bounds[3] - geneva_bounds[1],\n",
    "                                     linewidth=2, edgecolor='black', \n",
    "                                     facecolor='none', linestyle='-')\n",
    "    ax_main.add_patch(boundary_rect)\n",
    "    \n",
    "    # Set appropriate limits for Switzerland\n",
    "    swiss_bounds = cantons.total_bounds\n",
    "    ax_main.set_xlim(swiss_bounds[0] - 0.1, swiss_bounds[2] + 0.1)\n",
    "    ax_main.set_ylim(swiss_bounds[1] - 0.1, swiss_bounds[3] + 0.1)\n",
    "    ax_main.set_xlabel('Easting', fontsize=12)\n",
    "    ax_main.set_ylabel('Northing', fontsize=12)\n",
    "    ax_main.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add major Swiss cities for reference (approximate coordinates)\n",
    "    cities = {\n",
    "        'Bern': [2597542.079, 1199612.021],\n",
    "        'Zürich': [2682517.95, 1248415.856],\n",
    "        'Basel': [2611664.667, 1267460.265],\n",
    "        'Geneva': [2500062.992, 1118117.691]\n",
    "    }\n",
    "    \n",
    "    for city, coords in cities.items():\n",
    "        ax_main.plot(coords[0], coords[1], 'ko', markersize=6)\n",
    "        ax_main.annotate(city, coords, xytext=(5, -15), \n",
    "                        textcoords='offset points', fontsize=10,\n",
    "                        bbox=dict(boxstyle='round,pad=0.3', facecolor='white', alpha=0.8))\n",
    "    \n",
    "    # === GENEVA DETAIL MAP ===\n",
    "    # ax_detail.set_title('Canton of Geneva\\nMunicipalities', \n",
    "    #                    fontsize=12, fontweight='bold', pad=15)\n",
    "    \n",
    "    # Plot all Geneva municipalities\n",
    "    communes.plot(ax=ax_detail, color='lightblue', \n",
    "                 edgecolor='navy', linewidth=1, alpha=0.7)\n",
    "    \n",
    "    # Highlight Geneva city (most populated municipality) if found\n",
    "    if geneva_city is not None:\n",
    "        geneva_city.plot(ax=ax_detail, color='orange', \n",
    "                        edgecolor='darkorange', linewidth=2, alpha=0.8)\n",
    "    \n",
    "    # Add municipality labels for the largest/most important ones\n",
    "    # Calculate centroids for labeling\n",
    "    communes_with_centroids = communes.copy()\n",
    "    communes_with_centroids['centroid'] = communes_with_centroids.geometry.centroid\n",
    "    communes_with_centroids['x'] = communes_with_centroids.centroid.x\n",
    "    communes_with_centroids['y'] = communes_with_centroids.centroid.y\n",
    "    \n",
    "    # Label key municipalities (you can customize this list)\n",
    "    key_municipalities = ['Genève']\n",
    "    \n",
    "    for idx, row in communes_with_centroids.iterrows():\n",
    "        if row['NAME'] in key_municipalities:\n",
    "            ax_detail.annotate('Geneva', (row['x'], row['y']), \n",
    "                             fontsize=8, ha='center', va='center',\n",
    "                             bbox=dict(boxstyle='round,pad=0.2', \n",
    "                                     facecolor='white', alpha=0.8))\n",
    "    \n",
    "    # Set limits to Geneva bounds with small buffer\n",
    "    buffer = 0.01\n",
    "    ax_detail.set_xlim(geneva_bounds[0] - buffer, geneva_bounds[2] + buffer)\n",
    "    ax_detail.set_ylim(geneva_bounds[1] - buffer, geneva_bounds[3] + buffer)\n",
    "    ax_detail.set_xlabel('Easting', fontsize=10)\n",
    "    ax_detail.set_ylabel('Northing', fontsize=10)\n",
    "    ax_detail.grid(True, alpha=0.3)\n",
    "                \n",
    "    # Add basemap\n",
    "    # ctx.add_basemap(ax_detail, crs=communes.crs, source=ctx.providers.OpenStreetMap.Mapnik, alpha=0.5)\n",
    "    # add_north_arrow(ax_main)\n",
    "    # add_north_arrow(ax_detail)\n",
    "    add_scale_bar(ax_main)\n",
    "    add_scale_bar(ax_detail)\n",
    "\n",
    "    # === CONNECTION LINE ===\n",
    "    # Create connection line from main map boundary to detail map\n",
    "    # Get the connection points\n",
    "    geneva_center = [geneva_bounds[0] + (geneva_bounds[2] - geneva_bounds[0])/2, \n",
    "                    geneva_bounds[1] + (geneva_bounds[3] - geneva_bounds[1])/2]\n",
    "    \n",
    "    main_connection_point = (geneva_bounds[2], geneva_center[1])  # Right side of Geneva rectangle\n",
    "    detail_connection_point = (geneva_bounds[0], geneva_center[1])  # Left side of detail map\n",
    "    \n",
    "    # Create connection patch\n",
    "    con = ConnectionPatch(main_connection_point, detail_connection_point,\n",
    "                         \"data\", \"data\", axesA=ax_main, axesB=ax_detail,\n",
    "                         arrowstyle=\"->\", shrinkB=5, shrinkA=5,\n",
    "                         color=\"black\", linewidth=1.5)\n",
    "    ax_detail.add_artist(con)\n",
    "    \n",
    "    # === STYLING AND LAYOUT ===\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Add overall title\n",
    "    # fig.suptitle('Canton of Geneva in Switzerland - Study Area Context', \n",
    "    #             fontsize=16, fontweight='bold', y=0.95)\n",
    "    \n",
    "    # Add scale bars and north arrows if needed\n",
    "    # (You can add these using matplotlib-scalebar or custom functions)\n",
    "    \n",
    "    # Add legend\n",
    "    legend_elements = [\n",
    "        patches.Patch(color='red', alpha=0.7, label='Canton of Geneva'),\n",
    "        patches.Patch(color='lightblue', alpha=0.7, label='Geneva municipalities'),\n",
    "        patches.Patch(color='orange', alpha=0.8, label='City of Geneva'),\n",
    "        plt.Line2D([0], [0], color='black', linestyle='-', label='Study area boundary')\n",
    "    ]\n",
    "    ax_main.legend(handles=legend_elements, loc='upper right', \n",
    "                  bbox_to_anchor=(1.56, 0.5), fontsize=12)\n",
    "    \n",
    "    return fig, (ax_main, ax_detail)\n",
    "def add_north_arrow(ax, location=(0.9, 0.9)):\n",
    "    \"\"\"\n",
    "    Add a north arrow to the map (optional enhancement).\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    ax : matplotlib.axes.Axes  \n",
    "        The axes to add the north arrow to\n",
    "    location : tuple\n",
    "        Location of north arrow as (x, y) in axes coordinates\n",
    "    \"\"\"\n",
    "    # Add a simple north arrow\n",
    "    ax.annotate('N', xy=location, xycoords='axes fraction',\n",
    "                fontsize=16, fontweight='bold', ha='center', va='center',\n",
    "                bbox=dict(boxstyle='circle', facecolor='white', edgecolor='black'))\n",
    "    ax.annotate('↑', xy=(location[0], location[1]-0.03), xycoords='axes fraction',\n",
    "                fontsize=14, ha='center', va='center')\n",
    "\n",
    "def add_scale_bar(ax):\n",
    "    scalebar = ScaleBar(1, units=\"m\", location=\"lower right\")\n",
    "    ax.add_artist(scalebar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax_main, ax_detail) = create_geneva_context_map(cantons, communes)\n",
    "plt.savefig(result_folder/'geneva_context_map.png', dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "### BCS screening centers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_centre = gpd.read_file('../Data/Raw data/BC_ScreeningCenters.geojson',driver = 'GeoJSON')\n",
    "\n",
    "gdf_centre.crs = 4326\n",
    "\n",
    "gdf_centre['geometry'] = gpd.points_from_xy(gdf_centre.lon, gdf_centre.lat)\n",
    "\n",
    "gdf_centre = gdf_centre.to_crs(2056)\n",
    "\n",
    "gdf_centre['year_start'] = gdf_centre['year_start'].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "## Evolution of BCS participation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_participation.loc[gdf_participation['year_invit'].isin([1999,2000]), 'year_invit_2y'] = '2000'\n",
    "gdf_participation.loc[gdf_participation['year_invit'].isin([2001,2002]), 'year_invit_2y'] = '2002'\n",
    "gdf_participation.loc[gdf_participation['year_invit'].isin([2003,2004]), 'year_invit_2y'] = '2004'\n",
    "gdf_participation.loc[gdf_participation['year_invit'].isin([2005,2006]), 'year_invit_2y'] = '2006'\n",
    "gdf_participation.loc[gdf_participation['year_invit'].isin([2007,2008]), 'year_invit_2y'] = '2008'\n",
    "gdf_participation.loc[gdf_participation['year_invit'].isin([2009,2010]), 'year_invit_2y'] = '2010'\n",
    "gdf_participation.loc[gdf_participation['year_invit'].isin([2011,2012]), 'year_invit_2y'] = '2012'\n",
    "gdf_participation.loc[gdf_participation['year_invit'].isin([2013,2014]), 'year_invit_2y'] = '2014'\n",
    "gdf_participation.loc[gdf_participation['year_invit'].isin([2015,2016]), 'year_invit_2y'] = '2016'\n",
    "gdf_participation.loc[gdf_participation['year_invit'].isin([2017,2018]), 'year_invit_2y'] = '2018'\n",
    "gdf_participation.loc[gdf_participation['year_invit'].isin([2019,2020]), 'year_invit_2y'] = '2020'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_participation.groupby('year_invit_2y').size().to_clipboard()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_participation[gdf_participation.mammo==1].groupby('year_invit_2y').size().to_clipboard()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_invit_by_year = pd.DataFrame(gdf_participation.groupby('year_invit_2y').size()).reset_index()\n",
    "pal = sns.color_palette(\"Blues\", len(n_invit_by_year))\n",
    "rank = n_invit_by_year[0].argsort().argsort()   # http://stackoverflow.com/a/6266510/1628638\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(8, 8))\n",
    "chart = sns.barplot(x=\"year_invit_2y\", y=0, data=n_invit_by_year, hue='year_invit_2y', palette = np.array(pal)[rank], legend=False, ax = ax)\n",
    "chart.set_xticklabels(chart.get_xticklabels(),size = 8, rotation=45, horizontalalignment='right')\n",
    "chart.set_xlabel(\"Biennal period of invitation\",size = 12)\n",
    "chart.set_ylabel(\"Invitation count\",size = 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_mammo_by_year = pd.DataFrame(gdf_participation.groupby('year_invit_2y').mammo.sum()).reset_index()\n",
    "pal = sns.color_palette(\"Blues\", len(n_mammo_by_year))\n",
    "rank = n_mammo_by_year['mammo'].argsort().argsort()   # http://stackoverflow.com/a/6266510/1628638\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(8, 8))\n",
    "chart = sns.barplot(x=\"year_invit_2y\", y=\"mammo\", data=n_mammo_by_year, hue='year_invit_2y', palette = np.array(pal)[rank], legend=False, ax = ax)\n",
    "chart.set_xticklabels(chart.get_xticklabels(),size = 10, rotation=45, horizontalalignment='right')\n",
    "chart.set_xlabel(\"Biennal period of invitation\",size = 12)\n",
    "chart.set_ylabel('Mammography count',size = 12)\n",
    "show_values(chart, digits = 0, fontsize = 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_rate_by_year = pd.DataFrame(n_mammo_by_year.set_index('year_invit_2y')['mammo'].div(n_invit_by_year.set_index('year_invit_2y')[0]).mul(100)).reset_index()\n",
    "\n",
    "pal = sns.color_palette(\"Blues\", len(n_mammo_by_year))\n",
    "rank = raw_rate_by_year[0].argsort().argsort()   # http://stackoverflow.com/a/6266510/1628638\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(8, 8))\n",
    "chart = sns.barplot(x=\"year_invit_2y\", y=0, data=raw_rate_by_year, hue='year_invit_2y', palette = np.array(pal)[rank], legend=False, ax = ax)\n",
    "chart.set_xticklabels(chart.get_xticklabels(),size = 10, rotation=45, horizontalalignment='right')\n",
    "chart.set_xlabel(\"Biennal period of invitation\",size = 12)\n",
    "chart.set_ylabel('Participation rate (%)',size = 12)\n",
    "show_values(chart)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_mammo_by_year[n_mammo_by_year.year_invit_2y.isin(['2000','2002'])==False].mammo.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_participation[gdf_participation.year_invit_2y.isin(['2000','2002'])==False].groupby('year_invit_2y').mammo.sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_participation[gdf_participation.year_invit_2y.isin(['2000','2002'])==False].numerodossier.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_participation[(gdf_participation.year_invit_2y.isin(['2000','2002'])==False)&(gdf_participation.mammo==1)&(gdf_participation.mammoanterieure=='non')].groupby('year_invit_2y')['numerodossier'].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26",
   "metadata": {},
   "source": [
    "## Final data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_participation = pd.merge(gdf_participation,pd.get_dummies(gdf_participation['etatcivil']), left_index = True, right_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_participation['deprivation_pca_q5_txt'] = gdf_participation['deprivation_pca_q5'].map({0:\"0 - Least deprived\", 1:'1',2:'2',3:'3', 4: '4 - Most deprived'})\n",
    "gdf_participation['deprivation_pca_q5_txt'] = pd.Categorical(gdf_participation['deprivation_pca_q5_txt'], ordered = True, categories = ['0 - Least deprived','1','2','3','4 - Most deprived'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop duplicates caused by spatial join (some individuals intersects with multiple ha)\n",
    "gdf_participation = gdf_participation[~gdf_participation.uuid.duplicated()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_mammo_by_2year = pd.DataFrame(gdf_participation.groupby('year_invit_2y').mammo.sum()).reset_index()\n",
    "n_invit_by_2year = pd.DataFrame(gdf_participation.groupby('year_invit_2y').size()).reset_index()\n",
    "raw_rate_by_2year = pd.DataFrame(n_mammo_by_2year.set_index('year_invit_2y')['mammo'].div(n_invit_by_2year.set_index('year_invit_2y')[0]), columns = ['raw rate']).reset_index()\n",
    "# Calculate standard errors\n",
    "raw_rate_by_2year['se'] = np.sqrt(raw_rate_by_2year['raw rate'] * (1 - raw_rate_by_2year['raw rate']) / n_invit_by_2year[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_mammo_by_2year_age = pd.DataFrame(gdf_participation.groupby(['year_invit_2y','deprivation_pca_q5']).mammo.sum()).reset_index()\n",
    "n_invit_by_2year_age = pd.DataFrame(gdf_participation.groupby(['year_invit_2y','deprivation_pca_q5']).size()).reset_index()\n",
    "raw_rate_by_2year_age = pd.DataFrame(n_mammo_by_2year_age.set_index(['year_invit_2y','deprivation_pca_q5'])['mammo'].div(n_invit_by_2year_age.set_index(['year_invit_2y','deprivation_pca_q5'])[0]), columns = ['raw rate']).reset_index()\n",
    "# Calculate standard errors\n",
    "raw_rate_by_2year_age['se'] = np.sqrt(raw_rate_by_2year_age['raw rate'] * (1 - raw_rate_by_2year_age['raw rate']) / n_invit_by_2year_age[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_rate_by_2year_age[['raw rate','se']] = raw_rate_by_2year_age[['raw rate','se']].mul(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_participation['year_invit_2y_int'] = gdf_participation['year_invit_2y'].astype(int)\n",
    "gdf_participation['year_invit_2y_int'] = pd.to_datetime(gdf_participation['year_invit_2y_int'], format = '%Y')\n",
    "gdf_participation['mammo_100'] = gdf_participation['mammo'] *100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# _ha = pd.DataFrame(gdf_participation_ha.groupby(['RELI','year_invit']).mammo.sum()).reset_index()\n",
    "n_mammo_by_neighborhood = pd.DataFrame(gdf_participation.groupby(['nbid','year_invit_2y']).mammo.sum()).reset_index()\n",
    "n_invit_by_neighborhood = pd.DataFrame(gdf_participation.groupby(['nbid','year_invit_2y']).size()).reset_index()\n",
    "\n",
    "n_mammo_by_neighborhood.columns = ['nbid','year_invit','n_mammo']\n",
    "n_invit_by_neighborhood.columns = ['nbid','year_invit','n_invit']\n",
    "\n",
    "#f.pivot(index='foo', columns='bar', values='baz')\n",
    "n_mammo_by_neighborhood = n_mammo_by_neighborhood.pivot(index ='nbid',columns = 'year_invit',values = 'n_mammo').fillna(0).reset_index()\n",
    "n_invit_by_neighborhood = n_invit_by_neighborhood.pivot(index ='nbid',columns = 'year_invit',values = 'n_invit').fillna(0).reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35",
   "metadata": {},
   "source": [
    "## Final number checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_participation[gdf_participation.mammo > 0].numerodossier.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Overall mammography (> 0) rate is : \",round(gdf_participation[gdf_participation.numerodepistage > 0].numerodossier.nunique()/gdf_participation.numerodossier.nunique(), 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Overall participation rate is : \",round(gdf_participation[gdf_participation.mammo==1].shape[0]/gdf_participation.shape[0], 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39",
   "metadata": {},
   "source": [
    "## Temporal trends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Increase the figure size\n",
    "plt.figure(figsize=(6, 6))\n",
    "\n",
    "# Use a custom color palette\n",
    "\n",
    "sns.lineplot(x='year_invit_2y_int', y='mammo_100', data=gdf_participation[gdf_participation.year_invit_2y.isin(['2000','2002'])==False])\n",
    "markers = pd.DataFrame(gdf_participation[gdf_participation.year_invit_2y.isin(['2000','2002'])==False].groupby('year_invit_2y').mammo_100.mean()).reset_index()\n",
    "markers['year_invit_2y'] = markers['year_invit_2y'].astype('int32')\n",
    "# Add a title\n",
    "# plt.title('Participation Rate Evolution Over Time Stratified By SES Deprivation Index', fontsize=16)\n",
    "\n",
    "\n",
    "# Increase the font size of the labels\n",
    "plt.xlabel('Biennial interval', fontsize=12)\n",
    "plt.ylabel('Participation rate (%)', fontsize=12)\n",
    "\n",
    "\n",
    "# Setting x-ticks every 2 years\n",
    "years = mdates.YearLocator(2) \n",
    "ax = plt.gca()\n",
    "ax.xaxis.set_major_locator(years)\n",
    "\n",
    "# Add a grid to both axes\n",
    "ax.grid(True, color='grey', linestyle='--', linewidth=0.5, zorder=0)\n",
    "\n",
    "# Format x-tick labels as YYYY\n",
    "ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y'))\n",
    "\n",
    "ax.axvline(pd.to_datetime('2014-01-01'), color='darkgrey', linestyle='--', lw=1.4, zorder=0)  # Change to the exact date if needed\n",
    "ax.text(pd.to_datetime('2014-01-15'), ax.get_ylim()[1]*0.7, ' Program expanded to\\n women aged 70-74', color='grey')\n",
    "\n",
    "# Save the figure with a high resolution\n",
    "# plt.savefig('../Manuscript/Figures - EN/Bar plot - Participation rate by 2y - Neighborhood.png', dpi = 300, bbox_inches='tight')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax.get_ylim()[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Increase the figure size\n",
    "plt.figure(figsize=(6, 6))\n",
    "\n",
    "# Use a custom color palette\n",
    "palette = sns.color_palette(\"RdYlGn_r\", 5)\n",
    "\n",
    "sns.lineplot(x='year_invit_2y_int', y='mammo_100', hue='deprivation_pca_q5_txt', palette=palette, data=gdf_participation[gdf_participation.year_invit_2y.isin(['2000','2002'])==False], alpha=1, zorder=4)\n",
    "\n",
    "# Add a title\n",
    "# plt.title('Participation Rate Evolution Over Time Stratified By SES Deprivation Index', fontsize=16)\n",
    "\n",
    "# Increase the font size of the labels\n",
    "plt.xlabel('Biennial interval', fontsize=12)\n",
    "plt.ylabel('Participation rate (%)', fontsize=12)\n",
    "\n",
    "# Increase the font size of the legend and its title\n",
    "plt.legend(title='SES deprivation index (quintiles)', title_fontsize='13', fontsize='12')\n",
    "\n",
    "# Setting x-ticks every 2 years\n",
    "years = mdates.YearLocator(2) \n",
    "ax = plt.gca()\n",
    "ax.xaxis.set_major_locator(years)\n",
    "\n",
    "# Add a grid to both axes\n",
    "ax.grid(True, color='grey', linestyle='--', linewidth=0.5, zorder=0)\n",
    "\n",
    "# Format x-tick labels as YYYY\n",
    "ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y'))\n",
    "\n",
    "# Save the figure with a high resolution\n",
    "# plt.savefig('../Manuscript/Figures - EN/Bar plot - Participation rate by 2y and SES - Neighborhood.png', dpi = 300, bbox_inches='tight')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_participation['groupeage'] = gdf_participation['groupeage'].astype('str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 6))\n",
    "palette = sns.color_palette(\"Greens\", 5)\n",
    "\n",
    "chart = sns.lineplot(x='year_invit_2y_int', y='mammo_100', hue='groupeage', data=gdf_participation[gdf_participation.year_invit_2y.isin(['2000','2002'])==False])\n",
    "show_values(chart)\n",
    "# plt.title('Participation Rate Evolution Over Time Stratified By Age Group')\n",
    "plt.xlabel('Biennial interval', fontsize=12)\n",
    "plt.ylabel('Participation rate (%)', fontsize=12)\n",
    "plt.legend(title='Age group (years)', loc = 'lower right')\n",
    "# setting x-ticks every 2 years\n",
    "years = mdates.YearLocator(2) \n",
    "ax = plt.gca()\n",
    "ax.xaxis.set_major_locator(years)\n",
    "ax.grid(True, color='grey', linestyle='--', linewidth=0.5, zorder=0)\n",
    "# format x-tick labels as YYYY\n",
    "ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y'))\n",
    "# plt.savefig('../Manuscript/Figures - EN/Bar plot - Participation rate by 2y and age group - Neighborhood.png', dpi = 300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45",
   "metadata": {},
   "source": [
    "## Data Aggregation at the Swiss Neighborhood scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46",
   "metadata": {},
   "outputs": [],
   "source": [
    "microgis_data = gpd.read_parquet(data_folder/'Processed data'/'microgis_data_depriv.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47",
   "metadata": {},
   "outputs": [],
   "source": [
    "microgis_data = microgis_data[['nbid', 'ptot', 'ciqmd','deprivation_pca', 'deprivation_pca_q5','dmdrent','wo_tertiary_education','rpnch','radune','rad3tert', 'geometry']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48",
   "metadata": {},
   "outputs": [],
   "source": [
    "nbid_deprivation = microgis_data.set_index('nbid')['deprivation_pca'].to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Considering all invitations\n",
    "gdf_neighborhood = pd.DataFrame(gdf_participation.nbid.unique(), columns = ['nbid'])\n",
    "gdf_neighborhood = pd.merge(microgis_data, n_invit_by_neighborhood, on = 'nbid').merge(n_mammo_by_neighborhood, on = 'nbid', suffixes = ('_invit','_mammo'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_neighborhood['index_socio_q3'] = pd.qcut(gdf_neighborhood['deprivation_pca'], 3, labels = False)\n",
    "gdf_neighborhood.loc[gdf_neighborhood['index_socio_q3'] == 0,'index_socio_q3'] = 'Low'\n",
    "gdf_neighborhood.loc[gdf_neighborhood['index_socio_q3'] == 1,'index_socio_q3'] = 'Medium'\n",
    "gdf_neighborhood.loc[gdf_neighborhood['index_socio_q3'] == 2,'index_socio_q3'] = 'High'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_neighborhood['index_socio_q3'] = pd.Categorical(gdf_neighborhood['index_socio_q3'],categories = ['Low','Medium','High'], ordered = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_cl = {np.nan:'#bababa',\"High\":'#d7191c',\n",
    "3:'#fdae61',\n",
    "'Medium':'#ffffbf',\n",
    "1:'#abd9e9',\n",
    "\"Low\":'#2c7bb6'}\n",
    "hmap = colors.ListedColormap([dict_cl[i] for i in gdf_neighborhood['index_socio_q3'].sort_values().unique()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DROP 2000 and 2002 DATA TO BE SURE THEY AREN'T USED\n",
    "gdf_neighborhood = gdf_neighborhood.drop(['2000_invit','2002_invit','2000_mammo','2002_mammo'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_neighborhood['2006_invit'].plot.hist(bins=50)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55",
   "metadata": {},
   "source": [
    "## Mapping raw and adjusted rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56",
   "metadata": {},
   "outputs": [],
   "source": [
    "providers = ctx.providers.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs_year = []\n",
    "for year in range(2004,2022,2):\n",
    "    invit_year = str(year)+'_invit'\n",
    "    mammo_year = str(year)+'_mammo'\n",
    "\n",
    "    df_neighborhood_year = gdf_neighborhood[gdf_neighborhood[invit_year] > 0][['nbid',invit_year,mammo_year,'geometry']]\n",
    "    wnn16 = lps.weights.KNN(cKDTree(get_points_array(df_neighborhood_year.geometry.centroid)), 16)\n",
    "    wnn8 = lps.weights.KNN(cKDTree(get_points_array(df_neighborhood_year.geometry.centroid)), 8)\n",
    "    # wnn4 = lps.weights.KNN(cKDTree(get_points_array(df_neighborhood_year.geometry.centroid)), 4)\n",
    "\n",
    "    e = np.array(df_neighborhood_year[mammo_year])\n",
    "    b = np.array(df_neighborhood_year[invit_year])\n",
    "    ebs_rate = sm.Spatial_Empirical_Bayes(e, b, wnn8)\n",
    "    risk = sm.Excess_Risk(e, b)\n",
    "    df_neighborhood_year['tx_screening'] = df_neighborhood_year[mammo_year].div(df_neighborhood_year[invit_year]).mul(100)\n",
    "    df_neighborhood_year['ebs_rate'] = ebs_rate.r\n",
    "    df_neighborhood_year['ebs_rate'] = df_neighborhood_year['ebs_rate'].fillna(0)\n",
    "    df_neighborhood_year['ebs_rate'] = df_neighborhood_year['ebs_rate'].mul(100)\n",
    "    df_neighborhood_year['excess_risk'] = risk.r\n",
    "    df_neighborhood_year['excess_risk'] = df_neighborhood_year['excess_risk'].fillna(0)\n",
    "    df_neighborhood_year['er_bins'] = pd.cut(df_neighborhood_year['excess_risk'], bins = [0,0.25,0.5,0.75,1,1.25,2,4,10])\n",
    "    df_neighborhood_year['ebs_bins'] = pd.cut(df_neighborhood_year['ebs_rate'], bins = [0,10,20,30,40,50,60,80,100])\n",
    "    df_neighborhood_year['diff_rawrate_vs_ebsrate'] = df_neighborhood_year['tx_screening']-(df_neighborhood_year['ebs_rate'])\n",
    "    dfs_year.append(df_neighborhood_year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58",
   "metadata": {},
   "outputs": [],
   "source": [
    "year = 2004\n",
    "for df_year in dfs_year:\n",
    "    fig, ax = plt.subplots(figsize = (15,15))\n",
    "\n",
    "    df_year.plot('tx_screening',cmap = 'Blues', linewidth = 0.01, scheme = 'quantiles', legend = True,legend_kwds = {'title': \"Taux de participation (brut) (%)\",'fmt':\"{:.1f}\",'loc': 'upper left'},ax = ax)\n",
    "    communes.geometry.boundary.plot(linewidth = 0.3, color = 'grey', facecolor = None,ax = ax, zorder =0)\n",
    "    lake.plot(color = 'lightblue',ax = ax)\n",
    "    # ctx.add_basemap(ax, source=ctx.providers.OpenStreetMap.HOT,crs = 'EPSG:2056')\n",
    "    lake.apply(lambda x: ax.annotate(text=x.NOM, xy=x.geometry.centroid.coords[0], ha='center',size = 8),axis=1);\n",
    "    # communes.apply(lambda x: ax.annotate(text=x.NAME, xy=x.geometry.centroid.coords[0], ha='center',size = 6),axis=1);\n",
    "\n",
    "    # add scale bar\n",
    "    scalebar = ScaleBar(1, units=\"m\", location=\"lower right\")\n",
    "    ax.add_artist(scalebar)\n",
    "    ax.set_title('Raw participation rates to the breast cancer screening program %s'% str(year))\n",
    "    # ax.set_facecolor('black')\n",
    "    ax.set_axis_off()\n",
    "    filename = '%s.png' % str(year)\n",
    "    filepath = result_folder/'Rate maps'/'Raw'/filename\n",
    "    # plt.savefig(filepath,dpi = 300, bbox_inches='tight')\n",
    "    plt.clf()\n",
    "    plt.close(fig)\n",
    "    \n",
    "    year += 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59",
   "metadata": {},
   "outputs": [],
   "source": [
    "year = 2004\n",
    "for df_year in dfs_year:\n",
    "    fig, ax = plt.subplots(figsize = (15,15))\n",
    "    # ax = communes.plot(color = 'darkgrey',figsize = (15,15))\n",
    "\n",
    "    df_year.plot('ebs_rate',cmap = 'Blues', linewidth = 0.01, scheme = 'quantiles', legend = True,legend_kwds = {'title': \"Taux de participation (EBS) (%)\",'fmt':\"{:.1f}\",'loc': 'upper left'},ax = ax)\n",
    "    communes.geometry.boundary.plot(linewidth = 0.3, color = 'grey', facecolor = None,ax = ax, zorder =0)\n",
    "\n",
    "    lake.plot(color = 'lightblue',ax = ax)\n",
    "    # ctx.add_basemap(ax, source=ctx.providers.Stadia.StamenTonerLite,crs = 2056)\n",
    "    lake.apply(lambda x: ax.annotate(text=x.NOM, xy=x.geometry.centroid.coords[0], ha='center',size = 8),axis=1);\n",
    "    communes.apply(lambda x: ax.annotate(text=x.NAME, xy=x.geometry.centroid.coords[0], ha='center',size = 6),axis=1);\n",
    "\n",
    "    # add scale bar\n",
    "    scalebar = ScaleBar(1, units=\"m\", location=\"lower right\")\n",
    "    ax.add_artist(scalebar)\n",
    "    ax.set_title('SEBS participation rates to the breast cancer screening program in %s'% str(year))\n",
    "    # ax.set_facecolor('black')\n",
    "    ax.set_axis_off()\n",
    "    filename = '%s.png' % str(year)\n",
    "    filepath = result_folder/'Rate maps'/'Empirical Bayes Smoothing'/filename\n",
    "    # plt.savefig(filepath,dpi = 300, bbox_inches='tight')\n",
    "    plt.clf()\n",
    "    plt.close(fig)\n",
    "    \n",
    "    year += 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60",
   "metadata": {},
   "source": [
    "## Getis analyses - SEBS rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_year = 2004\n",
    "for df in dfs_year:\n",
    "    # Create K-nearest neighbors weights object using the centroids of geometries\n",
    "    weights_knn = lps.weights.KNN(cKDTree(get_points_array(df.geometry.centroid)), 16)\n",
    "\n",
    "    # Compute Getis Ord's statistics\n",
    "    getis_ord_stats = pyspace.compute_getis(df, 'ebs_rate', weights_knn, n_permut=9999, transform_type='B', p_001=True)\n",
    "\n",
    "    # Adjust p-values using the False Discovery Rate method\n",
    "    fdr_adjusted_p_value = fdr(getis_ord_stats.p_sim, alpha=0.05)\n",
    "    \n",
    "    # Create a new column for Getis Ord's classification with FDR adjustment\n",
    "    df['ebs_rate_G_cl_fdr'] = df['ebs_rate_G_cl'] \n",
    "    df.loc[df['ebs_rate_G_psim'] >= fdr_adjusted_p_value, 'ebs_rate_G_cl_fdr'] = 'Not significant'\n",
    "\n",
    "    # Plot Getis Ord's map\n",
    "    fig, ax = pyspace.plotGetisMap_ge(df, 'ebs_rate_G_cl', p_001=True, commune_name=False)\n",
    "    ax.set_title(start_year)\n",
    "\n",
    "    # Save the plot\n",
    "    filename = f'getis_giracs_ha_wnn16_005_{start_year}.png'\n",
    "    # plt.savefig(result_folder / 'Getis' / filename, dpi=300, bbox_inches='tight')\n",
    "\n",
    "    # Increase the start year by 2 for the next iteration\n",
    "    start_year += 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62",
   "metadata": {},
   "outputs": [],
   "source": [
    "colors_getis = ['#1c4978', '#2166ac', '#67a9cf', '#d1e5f0', '#991d2c', '#b2182b', '#ef8a62', '#fddbc7', '#c4bebe']\n",
    "colors_cl_getis = {'Cold Spot - p < 0.001':'#1c4978',\n",
    "                    'Cold Spot - p < 0.01':'#2166ac',\n",
    "                    'Cold Spot - p < 0.05':'#67a9cf',\n",
    "                    'Cold Spot - p < 0.1':'#d1e5f0',\n",
    "                    'Hot Spot - p < 0.001':'#991d2c',\n",
    "                    'Hot Spot - p < 0.01':'#b2182b',\n",
    "                    'Hot Spot - p < 0.05':'#ef8a62',\n",
    "                    'Hot Spot - p < 0.1':'#fddbc7',\n",
    "                    'Not significant':'#f7f7f7'}\n",
    "colors_cl = {'0 Non-Significant': 'lightgrey',\n",
    "             '1 High-High': '#d7191c',\n",
    "             '2 Low-High': '#abd9e9',\n",
    "             '3 Low-Low': '#2c7bb6',\n",
    "             '4 High-Low': '#fdae61'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize = (6, 6))\n",
    "chart = sns.barplot(x = 'ebs_rate_G_cl', y = 'ebs_rate', data = df_year, order=\n",
    "    ['Cold Spot - p < 0.001','Cold Spot - p < 0.01', 'Cold Spot - p < 0.05', 'Cold Spot - p < 0.1','Hot Spot - p < 0.001', 'Hot Spot - p < 0.01',\n",
    "     'Hot Spot - p < 0.05', 'Hot Spot - p < 0.1', 'Not significant'],\n",
    "                        palette=colors_getis, errorbar=('ci', 95),  ax = ax)\n",
    "chart.set_xlabel('Getis Classes', size=12)\n",
    "chart.set_ylabel('SEBS participation rates (%)', size=12)\n",
    "chart.set_xticklabels(chart.get_xticklabels(), size=8, rotation=45, horizontalalignment='right')\n",
    "\n",
    "plt.title('Average SEBS Participation Rates (%) by Getis Class', size = 12)\n",
    "# plt.savefig(figures_folder/'Bar plot - GIRACS -  Getis 2020.png', dpi = 300, bbox_inches = 'tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64",
   "metadata": {},
   "source": [
    "## Raw rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_year = 2004\n",
    "for df in dfs_year:\n",
    "    # Create K-nearest neighbors weights object using the centroids of geometries\n",
    "    weights_knn = lps.weights.KNN(cKDTree(get_points_array(df.geometry.centroid)), 16)\n",
    "\n",
    "    # Compute Getis Ord's statistics\n",
    "    getis_ord_stats = pyspace.compute_getis(df, 'tx_screening', weights_knn, n_permut=9999, transform_type='B', p_001=True)\n",
    "\n",
    "    # Adjust p-values using the False Discovery Rate method\n",
    "    fdr_adjusted_p_value = fdr(getis_ord_stats.p_sim, alpha=0.05)\n",
    "    \n",
    "    # Create a new column for Getis Ord's classification with FDR adjustment\n",
    "    df['tx_screening_G_cl_fdr'] = df['tx_screening_G_cl'] \n",
    "    df.loc[df['tx_screening_G_psim'] >= fdr_adjusted_p_value, 'tx_screening_G_cl_fdr'] = 'Not significant'\n",
    "\n",
    "    # Plot Getis Ord's map\n",
    "    fig, ax = pyspace.plotGetisMap_ge(df, 'tx_screening_G_cl', p_001=True, commune_name=False)\n",
    "    ax.set_title(start_year)\n",
    "\n",
    "    # Save the plot\n",
    "    filename = f'getis_giracs_ha_wnn16_005_raw_{start_year}.png'\n",
    "    # plt.savefig(result_folder / 'Getis' / filename, dpi=300, bbox_inches='tight')\n",
    "\n",
    "    # Increase the start year by 2 for the next iteration\n",
    "    start_year += 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66",
   "metadata": {},
   "source": [
    "## Temporal evolution of participation rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_years = pd.DataFrame()\n",
    "year = 2004\n",
    "for df_year in dfs_year:\n",
    "    df_year['year_invit'] = str(year)\n",
    "    df_all_years = pd.concat([df_all_years, df_year])\n",
    "    year +=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68",
   "metadata": {},
   "outputs": [],
   "source": [
    "nbid_2020 = df_all_years[df_all_years['2020_mammo'].isnull()==False].nbid.tolist()\n",
    "\n",
    "df_all_years_2020_neighborhood = df_all_years[(df_all_years.nbid.isin(nbid_2020))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_years_2020_neighborhood  = df_all_years_2020_neighborhood.set_index('nbid')\n",
    "df_all_years_2020_neighborhood = df_all_years_2020_neighborhood.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize = (15,8))\n",
    "# the size of A4 paper\n",
    "g = sns.lineplot(data=df_all_years_2020_neighborhood, x=\"year_invit\", y=\"ebs_rate\", hue = 'ebs_rate_G_cl',hue_order=\n",
    "    ['Cold Spot - p < 0.001','Cold Spot - p < 0.01', 'Cold Spot - p < 0.05', 'Cold Spot - p < 0.1','Hot Spot - p < 0.001', 'Hot Spot - p < 0.01',\n",
    "     'Hot Spot - p < 0.05', 'Hot Spot - p < 0.1', 'Not significant'], palette = colors_cl_getis,ax = ax)\n",
    "for l in g.lines:\n",
    "    y = l.get_ydata()\n",
    "    if len(y)>0:\n",
    "        g.annotate(f'{y[-1]:.1f}', xy=(0.97,y[-1]), xycoords=('axes fraction', 'data'), \n",
    "                     ha='left', va='center', color=l.get_color(), size = 12)\n",
    "        # g.annotate(f'{y[-9]:.1f}', xy=(0.19,y[-9]), xycoords=('axes fraction', 'data'), \n",
    "        #      ha='left', va='center', color=l.get_color(), size = 12)\n",
    "ax.set_xlabel('Biennial Intervals',labelpad = 10, size = 16)\n",
    "ax.set_ylabel('SEBS Participation Rates (%)',labelpad = 10, size = 16)\n",
    "plt.legend(prop={'size': 14})\n",
    "sns.despine()\n",
    "plt.xticks(size = 12)\n",
    "plt.yticks(size = 12)\n",
    "ax.grid(True, color='grey', linestyle='--', linewidth=0.5, zorder=0)\n",
    "\n",
    "# plt.title('Evolution du taux de participation au sein des clusters géographiques',size = 16)\n",
    "# plt.savefig('../Manuscript/Figures - EN/Temporal trend Getis - Neighborhood.png', dpi = 300, bbox_inches = 'tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize = (15,8))\n",
    "# the size of A4 paper\n",
    "g = sns.lineplot(data=df_all_years_2020_neighborhood, x=\"year_invit\", y=\"tx_screening\", hue = 'tx_screening_G_cl',hue_order=\n",
    "    ['Cold Spot - p < 0.001','Cold Spot - p < 0.01', 'Cold Spot - p < 0.05', 'Cold Spot - p < 0.1','Hot Spot - p < 0.001', 'Hot Spot - p < 0.01',\n",
    "     'Hot Spot - p < 0.05', 'Hot Spot - p < 0.1', 'Not significant'], palette = colors_cl_getis,ax = ax)\n",
    "for l in g.lines:\n",
    "    y = l.get_ydata()\n",
    "    if len(y)>0:\n",
    "        g.annotate(f'{y[-1]:.1f}', xy=(0.97,y[-1]), xycoords=('axes fraction', 'data'), \n",
    "                     ha='left', va='center', color=l.get_color(), size = 12)\n",
    "        # g.annotate(f'{y[-9]:.1f}', xy=(0.19,y[-9]), xycoords=('axes fraction', 'data'), \n",
    "        #      ha='left', va='center', color=l.get_color(), size = 12)\n",
    "ax.set_xlabel('Biennial Intervals',labelpad = 10, size = 16)\n",
    "ax.set_ylabel('Participation Rates (%)',labelpad = 10, size = 16)\n",
    "plt.legend(prop={'size': 14})\n",
    "sns.despine()\n",
    "plt.xticks(size = 12)\n",
    "plt.yticks(size = 12)\n",
    "ax.grid(True, color='grey', linestyle='--', linewidth=0.5, zorder=0)\n",
    "\n",
    "# plt.title('Evolution du taux de participation au sein des clusters géographiques',size = 16)\n",
    "# plt.savefig('../Manuscript/Figures/Evolution temporelle Getis - Raw rates.png', dpi = 300, bbox_inches = 'tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gdf_neighborhood['ebs_rate_2000'] = gdf_neighborhood.nbid.map(dfs_year[0].set_index(\"nbid\").ebs_rate.to_dict())\n",
    "# gdf_neighborhood['ebs_rate_2002'] = gdf_neighborhood.nbid.map(dfs_year[1].set_index(\"nbid\").ebs_rate.to_dict())\n",
    "gdf_neighborhood['ebs_rate_2004'] = gdf_neighborhood.nbid.map(dfs_year[0].set_index(\"nbid\").ebs_rate.to_dict())\n",
    "gdf_neighborhood['ebs_rate_2006'] = gdf_neighborhood.nbid.map(dfs_year[1].set_index(\"nbid\").ebs_rate.to_dict())\n",
    "gdf_neighborhood['ebs_rate_2008'] = gdf_neighborhood.nbid.map(dfs_year[2].set_index(\"nbid\").ebs_rate.to_dict())\n",
    "gdf_neighborhood['ebs_rate_2010'] = gdf_neighborhood.nbid.map(dfs_year[3].set_index(\"nbid\").ebs_rate.to_dict())\n",
    "gdf_neighborhood['ebs_rate_2012'] = gdf_neighborhood.nbid.map(dfs_year[4].set_index(\"nbid\").ebs_rate.to_dict())\n",
    "gdf_neighborhood['ebs_rate_2014'] = gdf_neighborhood.nbid.map(dfs_year[5].set_index(\"nbid\").ebs_rate.to_dict())\n",
    "gdf_neighborhood['ebs_rate_2016'] = gdf_neighborhood.nbid.map(dfs_year[6].set_index(\"nbid\").ebs_rate.to_dict())\n",
    "gdf_neighborhood['ebs_rate_2018'] = gdf_neighborhood.nbid.map(dfs_year[7].set_index(\"nbid\").ebs_rate.to_dict())\n",
    "gdf_neighborhood['ebs_rate_2020'] = gdf_neighborhood.nbid.map(dfs_year[8].set_index(\"nbid\").ebs_rate.to_dict())\n",
    "\n",
    "# gdf_neighborhood['ebs_rate_2000'] = gdf_neighborhood.nbid.map(dfs_year[0].set_index(\"nbid\").ebs_rate.to_dict())\n",
    "# gdf_neighborhood['ebs_rate_2002'] = gdf_neighborhood.nbid.map(dfs_year[1].set_index(\"nbid\").ebs_rate.to_dict())\n",
    "gdf_neighborhood['raw_rate_2004'] = gdf_neighborhood.nbid.map(dfs_year[0].set_index(\"nbid\").tx_screening.to_dict())\n",
    "gdf_neighborhood['raw_rate_2006'] = gdf_neighborhood.nbid.map(dfs_year[1].set_index(\"nbid\").tx_screening.to_dict())\n",
    "gdf_neighborhood['raw_rate_2008'] = gdf_neighborhood.nbid.map(dfs_year[2].set_index(\"nbid\").tx_screening.to_dict())\n",
    "gdf_neighborhood['raw_rate_2010'] = gdf_neighborhood.nbid.map(dfs_year[3].set_index(\"nbid\").tx_screening.to_dict())\n",
    "gdf_neighborhood['raw_rate_2012'] = gdf_neighborhood.nbid.map(dfs_year[4].set_index(\"nbid\").tx_screening.to_dict())\n",
    "gdf_neighborhood['raw_rate_2014'] = gdf_neighborhood.nbid.map(dfs_year[5].set_index(\"nbid\").tx_screening.to_dict())\n",
    "gdf_neighborhood['raw_rate_2016'] = gdf_neighborhood.nbid.map(dfs_year[6].set_index(\"nbid\").tx_screening.to_dict())\n",
    "gdf_neighborhood['raw_rate_2018'] = gdf_neighborhood.nbid.map(dfs_year[7].set_index(\"nbid\").tx_screening.to_dict())\n",
    "gdf_neighborhood['raw_rate_2020'] = gdf_neighborhood.nbid.map(dfs_year[8].set_index(\"nbid\").tx_screening.to_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73",
   "metadata": {},
   "outputs": [],
   "source": [
    "nbid_center_density = gdf_participation.groupby('nbid').center_density.mean().to_dict()\n",
    "nbid_center_nearest = gdf_participation.groupby('nbid').center_nearest.mean().to_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74",
   "metadata": {},
   "source": [
    "## Space-time cube clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymannkendall as mk\n",
    "from itertools import groupby\n",
    "from operator import itemgetter\n",
    "import utils\n",
    "from utils import translate_cat\n",
    "from utils import masked_emergent_getis\n",
    "import matplotlib.patches as mpatches\n",
    "from matplotlib.path import Path\n",
    "from matplotlib.patches import PathPatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_space_time_cube = pd.concat(dfs_year)[['nbid','year_invit','ebs_rate_G_Zs','ebs_rate_G_cl', 'tx_screening_G_Zs', 'tx_screening_G_cl','geometry']]\n",
    "df_space_time_cube['time'] = df_space_time_cube['year_invit'].astype(int).sub(2004).div(2)\n",
    "# df_space_time_cube.groupby('RELI').ebs_rate_G_cl.apply(list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_consecutive(lst, max_time):\n",
    "    consec_lsts = []\n",
    "    for k, g in groupby(enumerate(lst), lambda ix : ix[0] - ix[1]):\n",
    "        consecutive_list = list(map(itemgetter(1), g))\n",
    "#         print(consecutive_list)\n",
    "        consec_lsts.append(consecutive_list)\n",
    "#         print(len(consec_lsts))\n",
    "        if len(consec_lsts) > 1:\n",
    "            return np.nan\n",
    "        else:\n",
    "            if max_time in consecutive_list and len(consecutive_list)>1:\n",
    "                \n",
    "                return consecutive_list\n",
    "            else:\n",
    "                return np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_consecutive_v2(lst, max_time):\n",
    "    consec_lsts = []\n",
    "    for k, g in groupby(enumerate(lst), lambda ix: ix[0] - ix[1]):\n",
    "        consecutive_list = list(map(itemgetter(1), g))\n",
    "        consec_lsts.append(consecutive_list)\n",
    "    \n",
    "    # Find sequences that end in max_time and have length > 1\n",
    "    valid_sequences = [seq for seq in consec_lsts \n",
    "                      if max_time in seq and len(seq) > 1 and seq[-1] == max_time]\n",
    "    \n",
    "    # Should have exactly one valid sequence for \"consecutive\" pattern\n",
    "    if len(valid_sequences) == 1:\n",
    "        return valid_sequences[0]\n",
    "    else:\n",
    "        return np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def spatiotemporal_getis_cats_prep(df,class_col, z_col, max_time):\n",
    "    df_cats_prep = pd.DataFrame(pd.DataFrame(df.nbid.unique(), columns = ['nbid'])).set_index('nbid')\n",
    "    getis_cl_lst = df.groupby('nbid')[class_col].apply(list)\n",
    "    getis_Zs_lst = df.groupby('nbid')[z_col].apply(list)\n",
    "    getis_hs_index = getis_cl_lst.apply(lambda x: [i for i, s in enumerate(x) if 'Hot' in s])\n",
    "    getis_cs_index = getis_cl_lst.apply(lambda x: [i for i, s in enumerate(x) if 'Cold' in s])\n",
    "    df_cats_prep['n_periods'] = getis_cl_lst.apply(len)\n",
    "\n",
    "    df_cats_prep['hs_index'] = getis_hs_index\n",
    "    df_cats_prep['cs_index'] = getis_cs_index\n",
    "    df_cats_prep['all_na_cnt'] = getis_cl_lst.apply(lambda x: sum(pd.isnull(s) for s in x))\n",
    "    df_cats_prep['all_hs_cnt'] = getis_cl_lst.apply(lambda x: sum('Hot' in s for s in x))\n",
    "    df_cats_prep['all_cs_cnt'] = getis_cl_lst.apply(lambda x: sum('Cold' in s for s in x))\n",
    "    df_cats_prep['perc_hs'] = df_cats_prep['all_hs_cnt'].div(df['time'].nunique()).mul(100)\n",
    "    df_cats_prep['perc_cs'] = df_cats_prep['all_cs_cnt'].div(df['time'].nunique()).mul(100)\n",
    "\n",
    "    # Temporal trend\n",
    "    df_cats_prep['trend'] = getis_Zs_lst.apply(lambda x: mk.original_test(x).trend if len(x) > 1 else np.nan)\n",
    "    \n",
    "    # Consecutive hot spot, consecutive cold spot\n",
    "    df_cats_prep['consecutive_hs'] = getis_hs_index.apply(lambda x: get_consecutive_v2(x, max_time))\n",
    "    df_cats_prep['consecutive_cs'] = getis_cs_index.apply(lambda x: get_consecutive_v2(x, max_time))\n",
    "    df_cats_prep.loc[(df_cats_prep.hs_index.astype(str) == '[%s]'%(max_time)), 'class'] = 'Nouveau point chaud'\n",
    "    df_cats_prep.loc[(df_cats_prep.perc_hs < 90)&(df_cats_prep.consecutive_hs.isnull()==False), 'class'] = 'Point chaud consécutif'\n",
    "    df_cats_prep.loc[(df_cats_prep.perc_hs >= 90)&(df_cats_prep.consecutive_hs.isnull()==False)&(df_cats_prep.trend == 'increasing'), 'class'] = 'Intensification de point chaud'\n",
    "    df_cats_prep.loc[(df_cats_prep.perc_hs >= 90)&(df_cats_prep.consecutive_hs.isnull()==False)&(df_cats_prep.trend == 'no trend'), 'class'] = 'Point chaud persistant'\n",
    "    df_cats_prep.loc[(df_cats_prep.perc_hs >= 90)&(df_cats_prep.consecutive_hs.isnull()==False)&(df_cats_prep.trend == 'decreasing'), 'class'] = 'Point chaud diminuant'\n",
    "    df_cats_prep.loc[(df_cats_prep.perc_hs < 90)&(df_cats_prep.hs_index.astype(str).str.contains('%s]'%(max_time)))&(df_cats_prep.all_cs_cnt == 0)&(df_cats_prep.all_hs_cnt > 1), 'class'] = 'Point chaud sporadique'\n",
    "    df_cats_prep.loc[(df_cats_prep.perc_hs < 90)&(df_cats_prep.hs_index.astype(str).str.contains('%s]'%(max_time)))&(df_cats_prep.all_cs_cnt > 0), 'class'] = 'Point chaud oscillant'\n",
    "    df_cats_prep.loc[(df_cats_prep.perc_hs >= 50)&(df_cats_prep.hs_index.astype(str).str.contains('%s]'%(max_time)) == False), 'class'] = 'Point chaud historique'\n",
    "\n",
    "    df_cats_prep.loc[(df_cats_prep.cs_index.astype(str) == '[%s]'%(max_time)), 'class'] = 'Nouveau point froid'\n",
    "    df_cats_prep.loc[(df_cats_prep.perc_cs < 90)&(df_cats_prep.consecutive_cs.isnull()==False), 'class'] = 'Point froid consécutif'\n",
    "    df_cats_prep.loc[(df_cats_prep.perc_cs >= 90)&(df_cats_prep.consecutive_cs.isnull()==False)&(df_cats_prep.trend == 'decreasing'), 'class'] = 'Intensification de point froid'\n",
    "    df_cats_prep.loc[(df_cats_prep.perc_cs >= 90)&(df_cats_prep.consecutive_cs.isnull()==False)&(df_cats_prep.trend == 'no trend'), 'class'] = 'Point froid persistant'\n",
    "    df_cats_prep.loc[(df_cats_prep.perc_cs >= 90)&(df_cats_prep.consecutive_cs.isnull()==False)&(df_cats_prep.trend == 'increasing'), 'class'] = 'Point froid diminuant'\n",
    "    df_cats_prep.loc[(df_cats_prep.perc_cs < 90)&(df_cats_prep.cs_index.astype(str).str.contains('%s]'%(max_time)))&(df_cats_prep.all_hs_cnt == 0)&(df_cats_prep.all_cs_cnt > 1), 'class'] = 'Point froid sporadique'\n",
    "    df_cats_prep.loc[(df_cats_prep.perc_cs < 90)&(df_cats_prep.cs_index.astype(str).str.contains('%s]'%(max_time)))&(df_cats_prep.all_cs_cnt > 0), 'class'] = 'Point froid oscillant'\n",
    "    df_cats_prep.loc[(df_cats_prep.perc_cs >= 50)&(df_cats_prep.cs_index.astype(str).str.contains('%s]'%(max_time)) == False), 'class'] = 'Point froid historique'\n",
    "    df_cats_prep.loc[df_cats_prep['class'].isnull(),'class'] = 'Aucun modèle détecté'\n",
    "    return df_cats_prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_emergent_getis = spatiotemporal_getis_cats_prep(df_space_time_cube,'ebs_rate_G_cl','ebs_rate_G_Zs', 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_emergent_getis_test = spatiotemporal_getis_cats_prep(df_space_time_cube,'tx_screening_G_cl','tx_screening_G_Zs', 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_emergent_getis['class'] = translate_cat(df_emergent_getis['class'],'FR')\n",
    "df_emergent_getis_test['class'] = translate_cat(df_emergent_getis_test['class'],'FR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_emergent_getis_n9 = df_emergent_getis[df_emergent_getis.n_periods == 9] # Restricted to neighborhoods present all periods\n",
    "df_emergent_getis_test_n9 = df_emergent_getis_test[df_emergent_getis_test.n_periods == 9] # Restricted to neighborhoods present all periods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_emergent_getis['geometry'] = gdf_neighborhood.set_index('nbid')['geometry']\n",
    "df_emergent_getis_n9['geometry'] = gdf_neighborhood.set_index('nbid')['geometry']\n",
    "\n",
    "df_emergent_getis_test['geometry'] = gdf_neighborhood.set_index('nbid')['geometry']\n",
    "df_emergent_getis_test_n9['geometry'] = gdf_neighborhood.set_index('nbid')['geometry']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_emergent_getis = gpd.GeoDataFrame(df_emergent_getis, crs = 2056, geometry = df_emergent_getis['geometry'])\n",
    "df_emergent_getis_n9 = gpd.GeoDataFrame(df_emergent_getis_n9, crs = 2056, geometry = df_emergent_getis_n9['geometry'])\n",
    "\n",
    "df_emergent_getis_test = gpd.GeoDataFrame(df_emergent_getis_test, crs = 2056, geometry = df_emergent_getis_test['geometry'])\n",
    "df_emergent_getis_test_n9 = gpd.GeoDataFrame(df_emergent_getis_test_n9, crs = 2056, geometry = df_emergent_getis_test_n9['geometry'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86",
   "metadata": {},
   "outputs": [],
   "source": [
    "colors_cl_getis = { 'Nouveau point froid':'#eff3ff',\n",
    "                    'Point froid diminuant': \"#80cdc1\",\n",
    "                    'Point froid persistant':'#313695',\n",
    "                    'Point froid historique':'#2166ac',\n",
    "                    'Point froid sporadique':'#67a9cf',\n",
    "                    'Point froid oscillant':'#d1e5f0',\n",
    "                    'Intensification de point froid':'#036ffc',\n",
    "                    'Point chaud persistant':'#e41a1c',\n",
    "                    'Point chaud historique':'#b2182b',\n",
    "                    'Point chaud sporadique':'#ef8a62',\n",
    "                    'Point chaud oscillant':'#fddbc7',\n",
    "                    'Intensification de point chaud':'#e7298a',\n",
    "                    'Nouveau point chaud':'#ffeda0',\n",
    "                    'Point chaud diminuant': \"#fb9a99\",\n",
    "                    'Aucun modèle détecté':'#f0f0f0'}\n",
    "\n",
    "colors_cl_getis_en = {\n",
    "# 'New cold spot': '#eff3ff',\n",
    "'Diminishing cold spot': \"#80cdc1\",\n",
    "'Persistent cold spot': '#313695',\n",
    "'Historical cold spot': '#2166ac',\n",
    "# 'Sporadic cold spot': '#67a9cf',\n",
    "'Oscillating cold spot': '#d1e5f0',\n",
    "'Intensifying cold spot': '#036ffc',\n",
    "'Persistent hot spot': '#e41a1c',\n",
    "'Historical hot spot': '#b2182b',\n",
    "'Sporadic hot spot': '#ef8a62',\n",
    "'Oscillating hot spot': '#fddbc7',\n",
    "'Intensifying hot spot': '#e7298a',\n",
    "'New hot spot': '#ffeda0',\n",
    "'Diminishing hot spot': \"#fb9a99\",\n",
    "'No pattern detected': '#f0f0f0'\n",
    "}\n",
    "hmap = colors.ListedColormap([colors_cl_getis_en[i] for i in df_emergent_getis['class'].sort_values().unique()])\n",
    "# hmap = colors.ListedColormap([colors_cl_getis_en[i] for i in df_emergent_getis_test['class'].sort_values().unique()])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87",
   "metadata": {},
   "source": [
    "### Distribution of # of periods of presence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_by_n_periods = pd.DataFrame(df_emergent_getis.groupby('n_periods').size(), columns = ['n']).reset_index()\n",
    "perc_by_n_periods = pd.DataFrame(df_emergent_getis.groupby('n_periods').size(), columns = ['n']).reset_index()\n",
    "perc_by_n_periods['n'] = perc_by_n_periods['n']/21.12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the figure and the axes\n",
    "fig, ax = plt.subplots(figsize=(6, 6))\n",
    "sns.barplot(x='n_periods', y='n', data=n_by_n_periods, color = 'grey', ax=ax)\n",
    "# Add title and labels\n",
    "# plt.title('Nombre de Patients par Assureur', fontsize=16)\n",
    "plt.xlabel('Number of represented time intervals', fontsize=12)\n",
    "plt.ylabel('Number of Swiss Neighborhoods', fontsize=12)\n",
    "# Despine visual elements for a cleaner look\n",
    "sns.despine()\n",
    "ax.grid(True, color='grey', linestyle='--', linewidth=0.5, zorder=0)\n",
    "# plt.savefig('../Manuscript/Figures - EN/Distribution of represented periods - Neighborhoods.png', dpi = 300, bbox_inches = 'tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the figure and the axes\n",
    "fig, ax = plt.subplots(figsize=(6,6))\n",
    "sns.barplot(x='n_periods', y='n', data=perc_by_n_periods, color = 'grey', ax=ax)\n",
    "# Add title and labels\n",
    "# plt.title('Nombre de Patients par Assureur', fontsize=16)\n",
    "plt.xlabel('Number of represented time intervals', fontsize=12)\n",
    "plt.ylabel('Percentage of Swiss Neighborhoods', fontsize=12)\n",
    "# Despine visual elements for a cleaner look\n",
    "sns.despine()\n",
    "ax.grid(True, color='grey', linestyle='--', linewidth=0.5, zorder=0)\n",
    "# plt.savefig('../Manuscript/Figures - EN/Distribution of represented periods (%) - Neighborhoods.png', dpi = 300, bbox_inches = 'tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Person-neighborhood observations (corrected to exclude 2000, 2002)\n",
    "\n",
    "n_women_by_nbid = gdf_participation[gdf_participation.year_invit_2y.isin(['2000','2002'])==False].groupby('nbid')['numerodossier'].nunique()\n",
    "n_mammo_by_nbid = gdf_participation[gdf_participation.year_invit_2y.isin(['2000','2002'])==False].groupby('nbid')['mammo'].sum()\n",
    "\n",
    "df_emergent_getis['n_mammo'] = df_emergent_getis.index.map(n_mammo_by_nbid)\n",
    "df_emergent_getis['n_women_by_nbid'] = df_emergent_getis.index.map(n_women_by_nbid)\n",
    "\n",
    "\n",
    "samples = df_emergent_getis.groupby(['class']).n_women_by_nbid.sum().astype(str)\n",
    "total_samples = df_emergent_getis.n_women_by_nbid.sum()\n",
    "percentages = (df_emergent_getis.groupby('class').n_women_by_nbid.sum()/ total_samples).mul(100).round(1).astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_formatted = samples.astype(int).apply(lambda x: f'{x:,}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure 'samples' is a Series or something that can be used to sort 'df_emergent_getis'\n",
    "df_emergent_getis['samples_count'] = df_emergent_getis['class'].map(samples)\n",
    "\n",
    "# Sort the DataFrame based on the count of samples\n",
    "df_emergent_getis_sorted = df_emergent_getis.sort_values(by='samples_count', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_dict = samples.to_dict()\n",
    "# samples_dict['New cold spot'] = '0'\n",
    "# samples_dict['Sporadic cold spot'] = '0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # y = df_emergent_getis_sorted['class'] + ' - ' +  df_emergent_getis_sorted['class'].map(samples) + ' ('+ df_emergent_getis_sorted['class'].map(percentages) + '%)'\n",
    "# category_name = df_emergent_getis_sorted['class'].astype(str)\n",
    "# n_by_category = df_emergent_getis_sorted['class'].map(samples).astype(int)  # Keep as int first\n",
    "# n_by_category_formatted = n_by_category.apply(lambda x: f'{x:,}')  # Then format\n",
    "# perc_by_category = df_emergent_getis_sorted['class'].map(percentages).astype(str)\n",
    "\n",
    "# y = (category_name + ' - ' + \n",
    "#      n_by_category_formatted + \n",
    "#      ' (' + perc_by_category + '%)')\n",
    "\n",
    "# df_emergent_getis_sorted['plot_labels'] = y\n",
    "\n",
    "ax = df_emergent_getis_sorted.plot('class', cmap = hmap, figsize = (12,12), zorder = 3, linewidth = 0.01, legend = False)\n",
    "cantons[cantons.NAME == 'Genève'].geometry.boundary.plot(color = 'lightgrey', facecolor='None', ax = ax, zorder = 1)\n",
    "lake.plot(color = 'lightblue',ax = ax, zorder = 4)\n",
    "ax.set_axis_off()\n",
    "# communes.apply(lambda x: ax.annotate(text=x.NAME, xy=x.geometry.centroid.coords[0], ha='center', size=6, alpha = 0.8), axis=1)\n",
    "lake.apply(lambda x: ax.annotate(text=x.NOM, xy=x.geometry.centroid.coords[0], ha='center', size=12, alpha = 0.8, zorder=5), axis=1)\n",
    "\n",
    "\n",
    "# Add Geneva city boundary and annotation\n",
    "geneva_city = communes[communes.NAME == 'Genève']\n",
    "geneva_city.boundary.plot(ax=ax, color='black', linewidth=0.8, zorder = 3)\n",
    "\n",
    "communes.boundary.plot(ax=ax, color='black', linewidth=0.2, zorder = 3)\n",
    "\n",
    "# Add Geneva city label at centroid\n",
    "centroid = geneva_city.geometry.centroid.iloc[0]\n",
    "ax.annotate('Geneva', xy=(centroid.x, centroid.y), fontsize=12, fontweight='bold', \n",
    "            ha='center', va='center', zorder = 5)\n",
    "\n",
    "\n",
    "# legend = ax.get_legend()\n",
    "# # Change the fontsize\n",
    "# legend.get_title().set_fontsize(12)\n",
    "\n",
    "gdf_centre.plot(c = '#31a354', markersize = 40, alpha = 1, marker = 'x', ax = ax, zorder = 6)\n",
    "\n",
    "# Reorder legend by sample count (N)\n",
    "legend_data = []\n",
    "for category, color in colors_cl_getis_en.items():\n",
    "    count = samples_dict[category]  # Get N for this category\n",
    "    count = int(count)\n",
    "    legend_data.append((count, category, color))\n",
    "\n",
    "# Sort by count (descending)\n",
    "legend_data.sort(key=lambda x: x[0], reverse=True)\n",
    "\n",
    "# Create ordered legend elements\n",
    "legend_elements = []\n",
    "for count, category, color in legend_data:\n",
    "    legend_elements.append(patches.Patch(color=color, label=f'{category} - {count:,} ({count/total_samples*100:.1f}%)'))\n",
    "\n",
    "# Add screening centers to legend\n",
    "legend_elements.append(plt.Line2D([0], [0], marker='x', color='#31a354', linestyle='None', \n",
    "                                 markersize=10, markeredgewidth=3, label='Breast screening centers'))\n",
    "\n",
    "# Apply the ordered legend\n",
    "ax.legend(handles=legend_elements, loc='upper left', \n",
    "          fontsize=10, title='Categories - N women (%)', title_fontsize=11)\n",
    "scalebar = ScaleBar(1, units=\"m\", location=\"lower right\")\n",
    "ax.add_artist(scalebar)\n",
    "plt.savefig('../Manuscript/Figures - EN/Emergent Getis - Neighborhood - REVIEW.png', transparent=True, dpi = 400, bbox_inches = 'tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_emergent_getis_sorted['labels'] = df_emergent_getis_sorted['class'] + ' - ' +  df_emergent_getis_sorted['class'].map(samples_formatted) + ' ('+ df_emergent_getis_sorted['class'].map(percentages) + '%)'\n",
    "dict_labels = df_emergent_getis_sorted.set_index('class')['labels'].to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97",
   "metadata": {},
   "outputs": [],
   "source": [
    "for category in df_emergent_getis['class'].unique():\n",
    "    print(category)\n",
    "    utils.masked_emergent_getis(df_emergent_getis, gdf_centre, category, colors_cl_getis_en, dict_labels)\n",
    "    plt.savefig('../Manuscript/Figures - EN/Biennial_2004_2020_emergent_Getis_%s - Neighborhoods.png'%(category), dpi = 300, bbox_inches = 'tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_emergent_getis_n9['n_mammo'] = df_emergent_getis_n9.index.map(n_mammo_by_nbid)\n",
    "df_emergent_getis_n9['n_women_by_nbid'] = df_emergent_getis_n9.index.map(n_women_by_nbid)\n",
    "\n",
    "\n",
    "samples = df_emergent_getis_n9.groupby(['class']).n_women_by_nbid.sum().astype(str)\n",
    "total_samples = df_emergent_getis_n9.n_women_by_nbid.sum()\n",
    "percentages = (df_emergent_getis_n9.groupby('class').n_women_by_nbid.sum()/ total_samples).mul(100).round(1).astype(str)\n",
    "samples_dict = samples.to_dict()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpd.sjoin(gdf_participation[gdf_participation.year_invit_2y.isin(['2000','2002'])==False], df_emergent_getis_n9, predicate='within').numerodossier.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "100",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df_emergent_getis_n9['class'] + ' - ' +  df_emergent_getis_n9['class'].map(samples) + ' ('+ df_emergent_getis_n9['class'].map(percentages) + '%)'\n",
    "ax = df_emergent_getis_n9.plot(y, cmap = hmap, figsize = (12,12), zorder = 3, linewidth = 0.01, legend = True, legend_kwds = {'title':'Categories - N women (%)','fontsize':10, 'loc':'upper left'})\n",
    "# communes.plot(color = 'lightgrey', ax = ax, zorder = 1)\n",
    "lake.plot(color = 'lightblue',ax = ax, zorder = 4)\n",
    "ax.set_axis_off()\n",
    "# communes.apply(lambda x: ax.annotate(text=x.NAME, xy=x.geometry.centroid.coords[0], ha='center', size=6, alpha = 0.8), axis=1)\n",
    "lake.apply(lambda x: ax.annotate(text=x.NOM, xy=x.geometry.centroid.coords[0], ha='center', size=12, alpha = 0.8), axis=1)\n",
    "\n",
    "# Add Geneva city boundary and annotation\n",
    "geneva_city = communes[communes.NAME == 'Genève']\n",
    "geneva_city.boundary.plot(ax=ax, color='black', linewidth=0.8, zorder = 3)\n",
    "\n",
    "communes.boundary.plot(ax=ax, color='black', linewidth=0.2, zorder = 3)\n",
    "\n",
    "# Add Geneva city label at centroid\n",
    "centroid = geneva_city.geometry.centroid.iloc[0]\n",
    "ax.annotate('Geneva', xy=(centroid.x, centroid.y), fontsize=12, fontweight='bold', \n",
    "            ha='center', va='center', zorder = 5)\n",
    "\n",
    "\n",
    "\n",
    "gdf_centre.plot(c = '#31a354', markersize = 30, alpha = 1, marker = 'x', ax = ax, zorder = 6)\n",
    "\n",
    "\n",
    "# Reorder legend by sample count (N)\n",
    "legend_data = []\n",
    "for category, color in colors_cl_getis_en.items():\n",
    "    count = samples_dict[category]  # Get N for this category\n",
    "    count = int(count)\n",
    "    legend_data.append((count, category, color))\n",
    "\n",
    "# Sort by count (descending)\n",
    "legend_data.sort(key=lambda x: x[0], reverse=True)\n",
    "\n",
    "# Create ordered legend elements\n",
    "legend_elements = []\n",
    "for count, category, color in legend_data:\n",
    "    legend_elements.append(patches.Patch(color=color, label=f'{category} - {count:,} ({count/total_samples*100:.1f}%)'))\n",
    "\n",
    "# Add screening centers to legend\n",
    "legend_elements.append(plt.Line2D([0], [0], marker='x', color='#31a354', linestyle='None', \n",
    "                                 markersize=10, markeredgewidth=3, label='Breast screening centers'))\n",
    "\n",
    "# Apply the ordered legend\n",
    "ax.legend(handles=legend_elements, loc='upper left', \n",
    "          fontsize=10, title='Categories - N women (%)', title_fontsize=11)\n",
    "add_scale_bar(ax)\n",
    "plt.savefig('../Manuscript/Figures - EN/Emergent Getis - 9 periods only - Neighborhood.png', dpi = 300, bbox_inches = 'tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "101",
   "metadata": {},
   "source": [
    "### Descriptive analyses by SPTM class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "102",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_years_2020_neighborhood = df_all_years_2020_neighborhood.set_index('nbid')\n",
    "df_all_years_2020_neighborhood['class_emergent_getis'] = df_emergent_getis['class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "103",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_2020 = df_all_years_2020_neighborhood[df_all_years_2020_neighborhood.year_invit == '2020']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "104",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_2020['class_emergent_getis'] = data_2020['class_emergent_getis'].astype('string')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "105",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_context(\"paper\")\n",
    "flier_props = {'marker':'D', 'markersize':2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "106",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_participation_by_class(data, y_column, x_column, annotator_pairs, plot_type, output_file, colors_cl_getis_en, context=\"paper\", figsize=(6, 6), rotation=45, size=8):\n",
    "    from statannotations.Annotator import Annotator\n",
    "\n",
    "    sns.set_context(context)\n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "    colors_emergentGetis = [colors_cl_getis_en[i] for i in data[x_column].sort_values().unique()]\n",
    "    ylim_max = data[y_column].quantile(q=0.99)\n",
    "    x_label = 'Emerging hot spot categories'\n",
    "    if plot_type == 'barplot':\n",
    "        chart = sns.barplot(x=x_column, y=y_column, order = list([i for i in data[x_column].sort_values().unique()]),\n",
    "                        palette=colors_emergentGetis, data=data, ax=ax)\n",
    "    if plot_type == 'boxplot':\n",
    "        chart = sns.boxplot(x=x_column, y=y_column,order = list([i for i in data[x_column].sort_values().unique()]),\n",
    "                    palette=colors_emergentGetis, showfliers=True, flierprops=flier_props, data=data, ax=ax)\n",
    "    chart.set_xticklabels(chart.get_xticklabels(), size=size, rotation=rotation, horizontalalignment='right')\n",
    "    chart.set_xlabel(x_label, size=12)\n",
    "    chart.set_ylabel('SEBS Participation Rate (%)', size=12)\n",
    "\n",
    "    # plt.savefig(output_file, dpi = 300, bbox_inches = 'tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "107",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_participation_by_class(data_2020, 'ebs_rate', 'class_emergent_getis', [('No pattern detected',\"Persistent cold spot\"), ('No pattern detected','Persistent hot spot')],'boxplot', '../Manuscript/Figures - EN/Boxplot - Emergent Getis - Tx Participation - Neighborhood.png', colors_cl_getis_en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "108",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_years_2020_neighborhood.groupby(['year_invit','class_emergent_getis'])['ebs_rate'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "109",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize = (8, 8))\n",
    "# the size of A4 paper\n",
    "g = sns.lineplot(data=df_all_years_2020_neighborhood[df_all_years_2020_neighborhood.year_invit.isin(['2000','2002'])==False].reset_index(), x=\"year_invit\", y=\"ebs_rate\", hue = 'class_emergent_getis', linewidth = 2, palette = colors_cl_getis_en,ax = ax)\n",
    "for l in g.lines:\n",
    "    y = l.get_ydata()\n",
    "    if len(y)>0:\n",
    "        g.annotate(f'{y[-1]:.1f}', xy=(1.02,y[-1]), xycoords=('axes fraction', 'data'), \n",
    "                     ha='right', va='center', color=l.get_color(), size = 10)\n",
    "ax.set_xlabel('Biennial interval',labelpad = 10, size = 16)\n",
    "ax.set_ylabel('SEBS Participation Rate (%)',labelpad = 10, size = 16)\n",
    "plt.legend(prop={'size': 16})\n",
    "plt.xticks(size = 12)\n",
    "plt.yticks(size = 12)\n",
    "ax.grid(True, color='grey', linestyle='--', linewidth=0.5, zorder=0)\n",
    "\n",
    "# get the legend object\n",
    "leg = ax.legend()\n",
    "sns.despine()\n",
    "# change the line width for the legend\n",
    "for line in leg.get_lines():\n",
    "    line.set_linewidth(4.0)\n",
    "# plt.title('Evolution du taux de participation au sein des différentes catégories de clusters',size = 16)\n",
    "# plt.savefig('../Manuscript/Figures - EN/Emergent Getis - Evolution au cours du temps par classe - Neighborhood.png', dpi = 300, bbox_inches = 'tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "110",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_2020 = data_2020.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "111",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_2020['center_density'] = data_2020.nbid.map(nbid_center_density)\n",
    "data_2020['center_nearest'] = data_2020.nbid.map(nbid_center_nearest)\n",
    "\n",
    "data_2020 = pd.merge(data_2020, microgis_data.drop('geometry',axis=1), how='left', on = 'nbid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "112",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['nbid', '30-34', '35-39', '40-44', '45-49', '50-54', \n",
    "    '55-59', '60-64', '65-69', '70-74', '75-79', '80-84', '85-89', '90-94']\n",
    "\n",
    "# Now perform the selection and grouping\n",
    "_df_by_nbid = gdf_participation[columns].groupby('nbid').mean().astype(float).mul(100).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "113",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_2020 = pd.merge(data_2020, _df_by_nbid, how='left', on = 'nbid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "114",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Scale the column\n",
    "data_2020['deprivation_pca_scaled'] = scaler.fit_transform(-data_2020[['deprivation_pca']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "115",
   "metadata": {},
   "outputs": [],
   "source": [
    "colors_emergentGetis = [colors_cl_getis_en[i] for i in data_2020['class_emergent_getis'].sort_values().unique()]\n",
    "ylim_max = data_2020['ebs_rate'].quantile(q=0.99)\n",
    "x_label = ''\n",
    "# y_label = 'Taux de participation (%) en 2020'\n",
    "features = ['deprivation_pca_scaled','center_density']\n",
    "y_labels = ['SES deprivation index (scaled)','Access - Screening center density']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "116",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_2020.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "117",
   "metadata": {},
   "outputs": [],
   "source": [
    "for feature, y_label in zip(features, y_labels):\n",
    "    fig, ax = plt.subplots(figsize=(4, 4))\n",
    "#     f = plt.figure(figsize = (5,5))\n",
    "    chart = sns.boxplot(x='class_emergent_getis', y=feature,order = list([i for i in data_2020['class_emergent_getis'].sort_values().unique()]),\n",
    "                        palette=colors_emergentGetis, data=data_2020, flierprops = flier_props, ax=ax)\n",
    "    chart.set_xticklabels(chart.get_xticklabels(), size=8, rotation=45, horizontalalignment='right')\n",
    "    chart.set_title('{}'.format(y_label, size=16))\n",
    "    chart.set_xlabel(x_label, size=10)\n",
    "    chart.set_ylabel(y_label, size=10)\n",
    "    plt.tight_layout()\n",
    "    # plt.savefig(f'../Manuscript/Figures - EN/Box plot - Emergent Getis_{y_label} - Neighborhood.png', dpi = 300, bbox_inches = 'tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "118",
   "metadata": {},
   "source": [
    "## Modelling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "119",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import xgboost\n",
    "import shap\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "120",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_2020[['deprivation_pca','deprivation_pca_scaled']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "121",
   "metadata": {},
   "outputs": [],
   "source": [
    "old_names = ['ciqmd','center_density','center_nearest','deprivation_pca_scaled','dmdrent','wo_tertiary_education','rpnch','radune',\"ptot\",'rad3tert',\n",
    " '50-54',\n",
    " '55-59',\n",
    " '60-64',\n",
    " '65-69',\n",
    " '70-74',]\n",
    "\n",
    "new_names = ['Median income (CHF)','Center density','Nearest center','SES deprivation','Median monthly rent (CHF)','Tertiary education (%)','Swiss nationals (%)','Unemployment (%)',\"Population count\",'Tertiary sector (%)',\n",
    " '50-54',\n",
    " '55-59',\n",
    " '60-64',\n",
    " '65-69',\n",
    " '70-74']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "122",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_col_names = {k:v for k,v in zip(old_names,new_names)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "123",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_col_names['e'] = 'X-coord'\n",
    "dict_col_names['n'] = 'Y-coord'\n",
    "dict_col_names['ebs_rate'] = 'SEBS participation rate (%)'\n",
    "dict_col_names['tx_screening'] = 'Participation rate (%)'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "124",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_modelling = data_2020.copy()\n",
    "df_modelling.name = 'Participation'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "125",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_modelling['e'] = df_modelling.geometry.centroid.x\n",
    "df_modelling['n'] = df_modelling.geometry.centroid.y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "126",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_modelling = df_modelling.rename(columns=dict_col_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "127",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import hyperopt\n",
    "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials, space_eval\n",
    "\n",
    "# Fixed hyperparameter space\n",
    "space = {\n",
    "    'max_depth': hp.choice('max_depth', np.arange(3, 15, 1, dtype=int)),\n",
    "    'colsample_bytree': hp.quniform('colsample_bytree', 0.3, 1.0, 0.05),\n",
    "    'min_child_weight': hp.choice('min_child_weight', np.arange(1, 10, 1, dtype=int)),\n",
    "    'subsample': hp.quniform('subsample', 0.5, 1.0, 0.05),\n",
    "    'learning_rate': hp.quniform('learning_rate', 0.01, 0.3, 0.01),  # More realistic range\n",
    "    'gamma': hp.quniform('gamma', 0.0, 2.0, 0.1),\n",
    "    'reg_alpha': hp.quniform('reg_alpha', 0.0, 1.0, 0.1),    # L1 regularization\n",
    "    'reg_lambda': hp.quniform('reg_lambda', 1.0, 2.0, 0.1),  # L2 regularization\n",
    "    \n",
    "    'objective': 'reg:squarederror',\n",
    "    'eval_metric': 'rmse',\n",
    "}\n",
    "\n",
    "def score(params, X_train, y_train, n_folds=5):  # Pass data explicitly\n",
    "    \"\"\"Cross-validation scoring function\"\"\"\n",
    "    d_train = xgboost.DMatrix(X_train, y_train)\n",
    "    \n",
    "    cv_results = xgboost.cv(\n",
    "        params, d_train, \n",
    "        nfold=n_folds, \n",
    "        num_boost_round=500,\n",
    "        early_stopping_rounds=10, \n",
    "        metrics='rmse', \n",
    "        seed=0,\n",
    "        verbose_eval=False  # Reduce output\n",
    "    )\n",
    "    \n",
    "    loss = cv_results['test-rmse-mean'].min()\n",
    "    return loss\n",
    "\n",
    "def optimize(trials, space, n_evals, X_train, y_train):\n",
    "    \"\"\"Optimization function with data passed explicitly\"\"\"\n",
    "    \n",
    "    # Create a closure to pass data to score function\n",
    "    def objective(params):\n",
    "        return score(params, X_train, y_train)\n",
    "    \n",
    "    best = fmin(\n",
    "        objective, \n",
    "        space, \n",
    "        algo=tpe.suggest, \n",
    "        max_evals=n_evals,\n",
    "        trials=trials,\n",
    "        rstate=np.random.default_rng(333)\n",
    "    )\n",
    "    return best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "128",
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpretable_ml_shap_viz(df, y_col, final_model, X_coords, explainer_shap, shap_values, model_directory, include_spatial):\n",
    "    import warnings\n",
    "    # Suppress NumPy RNG warnings from SHAP\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.filterwarnings(\"ignore\", category=FutureWarning, message=\".*NumPy global RNG.*\")\n",
    "        \n",
    "        if not os.path.exists(model_directory):\n",
    "            os.makedirs(model_directory)\n",
    "        \n",
    "        # Shapley summary\n",
    "        shap.summary_plot(shap_values, feature_names=X_coords.columns, show=False)\n",
    "        plt.savefig(model_directory / 'shap_summary.png', dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        \n",
    "        # Shapley interaction values\n",
    "        shap_interaction_values = explainer_shap.shap_interaction_values(X_coords)\n",
    "        shap.summary_plot(shap_interaction_values, X_coords, max_display=16,\n",
    "                          feature_names=X_coords.columns, show=False, plot_type=\"compact_dot\")\n",
    "        plt.savefig(model_directory / 'shap_summary_interaction_values.png', dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        \n",
    "        # Shapley dependence plots\n",
    "        dependence_dir = model_directory / 'Dependence plots'\n",
    "        if not os.path.exists(dependence_dir):\n",
    "            os.makedirs(dependence_dir)\n",
    "        \n",
    "        for name in X_coords.columns:\n",
    "            shap.dependence_plot(name, shap_values.values, X_coords, display_features=X_coords, show=False)\n",
    "            plt.savefig(dependence_dir / f'shap_dependence_{name}.png', dpi=300, bbox_inches='tight')\n",
    "            plt.close()\n",
    "\n",
    "    # Maps of SHAP variables (spatial visualization)\n",
    "    if include_spatial:\n",
    "        n_columns = len(X_coords.columns)\n",
    "        n_rows = int(np.ceil(n_columns / 3))\n",
    "        \n",
    "        fig, ax = plt.subplots(n_rows, 3, figsize=(15, 15 * n_rows / 3))\n",
    "        ax = ax.ravel()\n",
    "\n",
    "        for j in range(n_columns):\n",
    "            df.plot(ax=ax[j], column=shap_values.values[:, j], legend=True,\n",
    "                    vmin=-0.8, vmax=0.8, cmap=shap.plots.colors.red_white_blue, legend_kwds={'shrink':0.5})\n",
    "            ax[j].set_title(\"SHAP for\\n\" + str(X_coords.columns[j]), fontsize=10)\n",
    "            ax[j].set_axis_off()\n",
    "\n",
    "        # Hide remaining unused subplots\n",
    "        for j in range(n_columns, n_rows * 3):\n",
    "            ax[j].set_axis_off()\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(model_directory / 'maps_shap_variables.png', dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "\n",
    "        # Map of location effects (assuming last 2 columns are X-coord, Y-coord)\n",
    "        spatial_shap = shap_values.values[:, -2:].sum(axis=1)\n",
    "        fig, ax = plt.subplots(dpi=300)\n",
    "        df.plot(ax=ax, column=spatial_shap, legend=True, vmin=-0.6, vmax=0.6, \n",
    "                figsize=(15, 8), cmap=shap.plots.colors.red_white_blue)\n",
    "        plt.title(\"Location Effect on Predictions\\n(SHAP values of geographic coordinates)\\n\", fontsize=8)\n",
    "        plt.axis('off')\n",
    "        plt.savefig(model_directory / 'maps_shap_location.png', dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "\n",
    "    # Variable importance\n",
    "    xgboost.plot_importance(final_model)\n",
    "    plt.title(\"Feature Importance (Gain)\")\n",
    "    plt.savefig(model_directory / 'importance_plot_default.png', dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "    xgboost.plot_importance(final_model, importance_type=\"cover\")\n",
    "    plt.title('Feature Importance (Cover)')\n",
    "    plt.savefig(model_directory / 'importance_plot_cover.png', dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "    # Force plot\n",
    "    shap.force_plot(explainer_shap.expected_value, shap_values.values[1, :], X_coords.iloc[1, :], \n",
    "                    show=False, matplotlib=True)\n",
    "    plt.savefig(model_directory / 'force_plot.png', dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "        \n",
    "    return df\n",
    "\n",
    "\n",
    "def train_xgboost(gwr_df, X_eq, y_col, optimize, space, n_evals, file_prefix, include_spatial=True, plot_figures=True):\n",
    "    df_model = gwr_df.copy(deep=True)\n",
    "    model_directory = result_folder / 'Modelling_REVIEW' / file_prefix\n",
    "    if not os.path.exists(model_directory):\n",
    "        os.makedirs(model_directory)\n",
    "    \n",
    "    # Prepare features\n",
    "    if include_spatial:\n",
    "        X_coords = df_model[X_eq + ['X-coord', 'Y-coord']]\n",
    "    else:\n",
    "        X_coords = df_model[X_eq]\n",
    "    \n",
    "    y = df_model[y_col]\n",
    "    \n",
    "    # Train-test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_coords, y, test_size=0.2, random_state=0)\n",
    "    \n",
    "    # Prepare DMatrix objects\n",
    "    d_train = xgboost.DMatrix(X_train, label=y_train)\n",
    "    d_test = xgboost.DMatrix(X_test, label=y_test)\n",
    "    d_all = xgboost.DMatrix(X_coords, label=y)\n",
    "    \n",
    "    ### Hyperparameter tuning\n",
    "    if not os.path.exists(model_directory / 'best_params.pkl'):\n",
    "        trials = Trials()\n",
    "        best_params = optimize(trials, space, n_evals=n_evals, X_train=X_train, y_train=y_train)\n",
    "        best_params = space_eval(space, best_params)\n",
    "        \n",
    "        with open(model_directory / 'trials.pkl', 'wb') as f:\n",
    "            pickle.dump(trials, f)\n",
    "        with open(model_directory / 'best_params.pkl', 'wb') as f:\n",
    "            pickle.dump(best_params, f)\n",
    "        \n",
    "        print(f\"Hyperparameter optimization completed for {file_prefix}\")\n",
    "    else:\n",
    "        with open(model_directory / 'best_params.pkl', 'rb') as f:\n",
    "            best_params = pickle.load(f)\n",
    "        print(f\"Loaded existing hyperparameters for {file_prefix}\")\n",
    "\n",
    "    ### Train model (single training approach)\n",
    "    if not os.path.exists(model_directory / 'model.pkl'):\n",
    "        final_model = xgboost.train(\n",
    "            best_params, \n",
    "            d_train, \n",
    "            num_boost_round=5000, \n",
    "            evals=[(d_train, \"train\"), (d_test, \"test\")], \n",
    "            verbose_eval=False, \n",
    "            early_stopping_rounds=10\n",
    "        )\n",
    "        with open(model_directory / 'model.pkl', 'wb') as f:\n",
    "            pickle.dump(final_model, f)\n",
    "    else:\n",
    "        with open(model_directory / 'model.pkl', 'rb') as f:\n",
    "            final_model = pickle.load(f)\n",
    "\n",
    "    ### Evaluation - separate test and full dataset\n",
    "    # Test set evaluation (unbiased performance)\n",
    "    y_pred_test = final_model.predict(d_test)\n",
    "    print(\"Test RMSE:\", np.sqrt(mean_squared_error(y_test, y_pred_test)))\n",
    "    print(\"Test R2:\", r2_score(y_test, y_pred_test))\n",
    "    \n",
    "    # Full dataset predictions (for visualization and SHAP)\n",
    "    y_pred_all = final_model.predict(d_all)\n",
    "    print('Full dataset RMSE:', np.sqrt(mean_squared_error(y, y_pred_all)))\n",
    "    print('Full dataset R2:', r2_score(y, y_pred_all))\n",
    "    \n",
    "    ### SHAP Analysis\n",
    "    explainer_shap = shap.TreeExplainer(final_model)\n",
    "    shap_values = explainer_shap(X_coords)\n",
    "    \n",
    "    # Debug SHAP calculation\n",
    "    print(f\"SHAP base_values shape: {np.array(shap_values.base_values).shape}\")\n",
    "    print(f\"SHAP values shape: {shap_values.values.shape}\")\n",
    "    print(f\"Expected predictions shape: {y_pred_all.shape}\")\n",
    "    \n",
    "    # Correct SHAP interpretation - handle scalar vs array base_values\n",
    "    if np.isscalar(shap_values.base_values):\n",
    "        # Single base value for all predictions\n",
    "        shap_predictions = shap_values.base_values + shap_values.values.sum(axis=1)\n",
    "    else:\n",
    "        # Array of base values (one per prediction)\n",
    "        shap_predictions = shap_values.base_values + shap_values.values.sum(axis=1)\n",
    "    \n",
    "    # Alternative method: use shap_values.data if available\n",
    "    if hasattr(shap_values, 'data') and shap_values.data is not None:\n",
    "        # Direct prediction from SHAP explanation\n",
    "        shap_predictions_alt = explainer_shap.expected_value + shap_values.values.sum(axis=1)\n",
    "        print(f\"Using explainer.expected_value: {explainer_shap.expected_value}\")\n",
    "        shap_predictions = shap_predictions_alt\n",
    "    \n",
    "    # Add results to dataframe\n",
    "    df_model['predictions'] = y_pred_all\n",
    "    df_model['shap_predictions'] = shap_predictions\n",
    "    df_model['residuals'] = df_model[y_col] - df_model['predictions']\n",
    "    \n",
    "    # Verify SHAP calculations with detailed debugging\n",
    "    shap_diff = np.abs(df_model['predictions'] - df_model['shap_predictions'])\n",
    "    max_diff = shap_diff.max()\n",
    "    mean_diff = shap_diff.mean()\n",
    "    \n",
    "    print(f\"SHAP vs XGB prediction differences:\")\n",
    "    print(f\"  Max difference: {max_diff:.6f}\")\n",
    "    print(f\"  Mean difference: {mean_diff:.6f}\")\n",
    "    print(f\"  Std difference: {shap_diff.std():.6f}\")\n",
    "    \n",
    "    if max_diff > 1e-3:  # More lenient threshold\n",
    "        print(f\"Warning: SHAP predictions have notable differences from XGB predictions\")\n",
    "        print(\"This might be normal for complex models with interactions\")\n",
    "    else:\n",
    "        print(\"✓ SHAP predictions match XGB predictions within tolerance\")\n",
    "    \n",
    "    ### Generate visualizations\n",
    "    if plot_figures:\n",
    "        df_model = interpretable_ml_shap_viz(df_model, y_col, final_model, X_coords, \n",
    "                                           explainer_shap, shap_values, model_directory, include_spatial)\n",
    "\n",
    "    return final_model, df_model, shap_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "129",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_directory = result_folder / 'Modelling' / 'Model 0'\n",
    "# if not os.path.exists(model_directory):\n",
    "#     os.makedirs(model_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "130",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_col = 'Participation rate (%)'  # Keep this name for column reference\n",
    "X_eq0 = ['Median income (CHF)','Center density','Median monthly rent (CHF)','Tertiary education (%)','Unemployment (%)','Tertiary sector (%)','Swiss nationals (%)',\"Population count\",\n",
    " '50-54','55-59','60-64','65-69','70-74']\n",
    "X_eq1 = ['SES deprivation','Center density', '50-54','55-59','60-64','65-69','70-74']\n",
    "X_eq2 = ['SES deprivation']\n",
    "X_eq3 = ['Center density']\n",
    "X_eq4 = ['Nearest center']\n",
    "\n",
    "include_spatial = True\n",
    "n_evals = 200\n",
    "df_model = df_modelling.copy(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "131",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_eq0 = ['Median income (CHF)','Center density','Median monthly rent (CHF)',\n",
    "         'Tertiary education (%)','Unemployment (%)','Tertiary sector (%)',\n",
    "         'Swiss nationals (%)',\"Population count\",\n",
    "         '50-54','55-59','60-64','65-69','70-74']\n",
    "\n",
    "X_eq1 = ['SES deprivation','Center density', \n",
    "         '50-54','55-59','60-64','65-69','70-74']\n",
    "\n",
    "X_eq2 = ['SES deprivation']\n",
    "\n",
    "X_eq3 = ['Center density']\n",
    "\n",
    "X_eq4 = ['Nearest center']\n",
    "\n",
    "# Common parameters\n",
    "y_col = 'Participation rate (%)'\n",
    "n_evals = 200\n",
    "\n",
    "# Verify that spatial coordinates exist in the dataframe\n",
    "required_coords = ['X-coord', 'Y-coord']\n",
    "if not all(coord in df_model.columns for coord in required_coords):\n",
    "    raise ValueError(f\"Missing spatial coordinates. Required: {required_coords}\")\n",
    "\n",
    "# Verify that all required features exist\n",
    "all_features = set(X_eq0 + X_eq1 + X_eq2 + X_eq3 + X_eq4)\n",
    "missing_features = [f for f in all_features if f not in df_model.columns]\n",
    "if missing_features:\n",
    "    print(f\"Warning: The following features are missing from the dataframe: {missing_features}\")\n",
    "    print(\"Available columns containing 'density' or 'center':\")\n",
    "    density_center_cols = [col for col in df_model.columns if 'density' in col.lower() or 'center' in col.lower()]\n",
    "    print(density_center_cols)\n",
    "\n",
    "print(\"Starting model training...\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Model configurations\n",
    "model_configs = [\n",
    "    # Spatial models\n",
    "    # {'features': X_eq0, 'name': 'Model 0', 'spatial': True, \n",
    "    #  'description': 'Full feature set with spatial'},\n",
    "    \n",
    "    {'features': X_eq1, 'name': 'Model 1 - Neighborhood - SEBS rates', 'spatial': True, 'outcome':'SEBS participation rate (%)',\n",
    "     'description': 'Reduced feature set with spatial'},\n",
    "    \n",
    "    {'features': X_eq1, 'name': 'Model 1 - Neighborhood - Raw rates', 'spatial': True, 'outcome':'Participation rate (%)',\n",
    "     'description': 'Reduced feature set with spatial - Raw participation rates'},\n",
    "    \n",
    "    # {'features': X_eq3, 'name': 'Model 3', 'spatial': True, \n",
    "    #  'description': 'Center density only with spatial'},\n",
    "    \n",
    "    # {'features': X_eq4, 'name': 'Model 4', 'spatial': False,  # Fixed: X_eq4 doesn't work with spatial\n",
    "    #  'description': 'Nearest center only (non-spatial)'},\n",
    "    \n",
    "    # Non-spatial models\n",
    "    {'features': X_eq1, 'name': 'Model 1 - Neighborhood - SEBS rates - Not spatial', 'spatial': False, 'outcome':'SEBS participation rate (%)',\n",
    "     'description': 'SES only without spatial'},\n",
    "    \n",
    "    # {'features': X_eq3, 'name': 'Model 3 - Not spatial', 'spatial': False, \n",
    "    #  'description': 'Center density only without spatial'},\n",
    "    \n",
    "    # {'features': X_eq4, 'name': 'Model 4 - Not spatial', 'spatial': False, \n",
    "    #  'description': 'Nearest center only without spatial'},\n",
    "]\n",
    "\n",
    "# Train all models\n",
    "models = {}\n",
    "results = {}\n",
    "\n",
    "for i, config in enumerate(model_configs):\n",
    "    print(f\"\\nTraining {config['name']}: {config['description']}\")\n",
    "    print(f\"Features: {config['features']}\")\n",
    "    print(f\"Spatial: {config['spatial']}\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    try:\n",
    "        # Verify feature availability\n",
    "        missing_features = [f for f in config['features'] if f not in df_modelling.columns]\n",
    "        if missing_features:\n",
    "            print(f\"Warning: Missing features {missing_features} for {config['name']}\")\n",
    "            print(\"Skipping this model...\")\n",
    "            continue\n",
    "            \n",
    "        # Train model\n",
    "        model, gwr_df, shap_values = train_xgboost(\n",
    "            gwr_df=df_modelling, \n",
    "            X_eq=config['features'], \n",
    "            y_col=config['outcome'], \n",
    "            optimize=optimize, \n",
    "            space=space, \n",
    "            n_evals=n_evals, \n",
    "            file_prefix=config['name'],\n",
    "            include_spatial=config['spatial'], \n",
    "            plot_figures=True\n",
    "        )\n",
    "        \n",
    "        # Store results\n",
    "        models[config['name']] = {\n",
    "            'model': model,\n",
    "            'dataframe': gwr_df,\n",
    "            'shap_values': shap_values,\n",
    "            'config': config\n",
    "        }\n",
    "        \n",
    "        print(f\"✓ {config['name']} completed successfully\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"✗ Error training {config['name']}: {str(e)}\")\n",
    "        import traceback\n",
    "        print(\"Full error traceback:\")\n",
    "        traceback.print_exc()\n",
    "        continue\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Model training completed!\")\n",
    "print(f\"Successfully trained {len(models)} models:\")\n",
    "for name in models.keys():\n",
    "    print(f\"  - {name}\")\n",
    "\n",
    "# Optionally, extract individual model results for backward compatibility\n",
    "if 'Model 0' in models:\n",
    "    model0 = models['Model 0']['model']\n",
    "    df_model0 = models['Model 0']['dataframe'] \n",
    "    shap_values_model0 = models['Model 0']['shap_values']\n",
    "\n",
    "if 'Model 1' in models:\n",
    "    model1 = models['Model 1']['model']\n",
    "    df_model1 = models['Model 1']['dataframe']\n",
    "    shap_values_model1 = models['Model 1']['shap_values']\n",
    "\n",
    "if 'Model 2' in models:\n",
    "    model2 = models['Model 2']['model']\n",
    "    df_model2 = models['Model 2']['dataframe']\n",
    "    shap_values_model2 = models['Model 2']['shap_values']\n",
    "\n",
    "if 'Model 3' in models:\n",
    "    model3 = models['Model 3']['model']\n",
    "    df_model3 = models['Model 3']['dataframe']\n",
    "    shap_values_model3 = models['Model 3']['shap_values']\n",
    "\n",
    "if 'Model 4' in models:\n",
    "    model4 = models['Model 4']['model']\n",
    "    df_model4 = models['Model 4']['dataframe']\n",
    "    shap_values_model4 = models['Model 4']['shap_values']\n",
    "\n",
    "if 'Model 2 - Not spatial' in models:\n",
    "    model2_bis = models['Model 2 - Not spatial']['model']\n",
    "    df_model2_bis = models['Model 2 - Not spatial']['dataframe']\n",
    "    shap_values_model2_bis = models['Model 2 - Not spatial']['shap_values']\n",
    "\n",
    "if 'Model 3 - Not spatial' in models:\n",
    "    model3_bis = models['Model 3 - Not spatial']['model']\n",
    "    df_model3_bis = models['Model 3 - Not spatial']['dataframe']\n",
    "    shap_values_model3_bis = models['Model 3 - Not spatial']['shap_values']\n",
    "\n",
    "if 'Model 4 - Not spatial' in models:\n",
    "    model4_bis = models['Model 4 - Not spatial']['model']\n",
    "    df_model4_bis = models['Model 4 - Not spatial']['dataframe']\n",
    "    shap_values_model4_bis = models['Model 4 - Not spatial']['shap_values']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11",
   "language": "python",
   "name": "py311"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
